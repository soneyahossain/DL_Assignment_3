{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_assignment_spring2020.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z-__RAO5t53G",
        "Y5AUSc3jufKi",
        "PGdZ-bmxb473",
        "rL8RK38atNE5",
        "ajJ1z9qb0oqt",
        "L_8eQuKBi7u0",
        "HpPb-CRh47Ct",
        "tAMnCGwv-zJN",
        "LGIxg3q5i_bl",
        "gXlKZ2eQj9Y7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c7cf0ce507d4e62a2205dfab8f2c1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8013aa04f9e146fb9e06ed13d4c99e6c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_17bf13abdcb0459a990b9558b5890c84",
              "IPY_MODEL_90f92f65ec194635a9d1d9878879cfe4"
            ]
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-__RAO5t53G",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning Basics\n",
        "\n",
        "In this tutorial we will implement a toy deep learning framework, to rapidly create neural network models. It should have some flexibilty and will be implemented so that new modules can be added and replaced just like in modern deep learning frameworks such as pytorch. We will be implementing some types of basic layers. We will also explore pytorch's nn package and see how pytorch accomplishes this goal and what are the extras that pytorch offers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5AUSc3jufKi",
        "colab_type": "text"
      },
      "source": [
        "### 1. Cleaner mini-batch-capable Linear + Softmax classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vby-ogCCxYwa",
        "colab_type": "text"
      },
      "source": [
        "In the previous assignment, we provided a basic implementation of a linear softmax classifier. In this assignment we will use this as a starting point for our deep learning library. \n",
        "\n",
        "**Softmax + Negative Log-Likelihood:** First we will re-implement the (softmax + negative log likelihood loss) computation, and its gradient computation. But we will additionally support batches of inputs and labels. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0JJRfD6ti7m",
        "colab_type": "code",
        "outputId": "cecf276e-db23-4a2d-a5b2-f97c3a88cecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# This class combines Softmax + Negative-log likelihood loss.\n",
        "# Similar to the previous lab, but this implementation works for \n",
        "# batches of inputs and not just individual input vectors. \n",
        "# Here \"inputs\" is batchSize x sizePredictionScores, and\n",
        "#      \"labels\" is a vector of size batchSize.\n",
        "class toynn_CrossEntropyLoss(object): \n",
        "  \n",
        "    # Forward pass: -log softmax(input_{label})\n",
        "    def forward(self, scores, labels):\n",
        "      \n",
        "        # 1. Computing the softmax: exp(x) / sum (exp(x))\n",
        "        max_val = scores.max()  # This is to avoid variable overflows.\n",
        "        exp_inputs = (scores - max_val).exp()\n",
        "        # This is different than in the previous lab. Avoiding for loops here.\n",
        "        denominators = exp_inputs.sum(1).repeat(scores.size(1), 1).t()\n",
        "        self.predictions = torch.mul(exp_inputs, 1 / denominators)\n",
        "        \n",
        "        # 2. Computing the loss: -log(y_label).\n",
        "        # Check what gather does. Just avoiding another for loop here.\n",
        "        return -self.predictions.log().gather(1, labels.view(-1, 1)).mean()\n",
        "    \n",
        "    # Backward pass: y_hat - y\n",
        "    def backward(self, scores, labels):\n",
        "      \n",
        "        # Here we avoid computing softmax again in backward pass.\n",
        "        grad_inputs = self.predictions.clone()\n",
        "        \n",
        "        # Ok, Here we will use a for loop (but it is avoidable too).\n",
        "        for i in range(0, scores.size(0)):\n",
        "            grad_inputs[i][labels[i]] = grad_inputs[i][labels[i]] - 1\n",
        "            \n",
        "        return grad_inputs \n",
        "      \n",
        "      \n",
        "# Let's verify if the above seems to be Okay.\n",
        "batchSize = 32\n",
        "mock_scores = torch.zeros(batchSize, 10).normal_(0, 0.1)\n",
        "mock_labels = torch.zeros(batchSize, 1, dtype=torch.long).fill_(3)\n",
        "\n",
        "loss_fn = toynn_CrossEntropyLoss()\n",
        "\n",
        "loss = loss_fn.forward(mock_scores, mock_labels)\n",
        "mock_scores_grads = loss_fn.backward(mock_scores, mock_labels)\n",
        "\n",
        "print(\"Input predictions: \", mock_scores.shape)\n",
        "print(\"Input labels: \", mock_labels.shape)\n",
        "print(\"Output loss: \", loss.item())\n",
        "print(\"Input prediction gradients: \", mock_scores_grads.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input predictions:  torch.Size([32, 10])\n",
            "Input labels:  torch.Size([32, 1])\n",
            "Output loss:  2.3062102794647217\n",
            "Input prediction gradients:  torch.Size([32, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DfaJM7RWk_0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Linear Transformation:**  Next, we will re-implement the linear transformation computation $y=Wx+b$, and its gradient computation. But we will additionally support batches of inputs and labels. Making a batched implementation of this layer is easier because the only change is that now we have matrix-matrix multiplications as opposed to vector-matrix multiplications. Additionally, we will support returning the gradients with respect to the inputs to the linear transformation ($\\partial \\ell / \\partial x_j$). Notice that in our previous assignment we were only concerned with computing $\\partial \\ell / \\partial w_{ij}$ and  $\\partial \\ell / \\partial b_i$ (gradients for the parameters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lfp8zLNXVo_",
        "colab_type": "code",
        "outputId": "ad453aec-facf-485e-b95a-90b236b36c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "class toynn_Linear(object):\n",
        "    def __init__(self, numInputs, numOutputs):\n",
        "        # Allocate tensors for the weight and bias parameters.\n",
        "        self.weight = torch.Tensor(numInputs, numOutputs).normal_(0, 0.01)\n",
        "        self.weight_grads = torch.Tensor(numInputs, numOutputs)\n",
        "        self.bias = torch.Tensor(numOutputs).zero_()\n",
        "        self.bias_grads = torch.Tensor(numOutputs)\n",
        "    \n",
        "    # Forward pass, inputs is a matrix of size batchSize x numInputs.\n",
        "    # Notice that compared to the previous assignment, each input vector\n",
        "    # is a row in this matrix.\n",
        "    def forward(self, inputs):\n",
        "        # This one needs no change, it just becomes \n",
        "        # a matrix x matrix multiplication\n",
        "        # as opposed to just vector x matrix multiplication as we had before.\n",
        "        return torch.matmul(inputs, self.weight) + self.bias\n",
        "    \n",
        "    # Backward pass, in addition to compute gradients for the weight and bias.\n",
        "    # It has to compute gradients with respect to inputs. \n",
        "    def backward(self, inputs, scores_grads):\n",
        "        self.weight_grads = torch.matmul(inputs.t(), scores_grads)\n",
        "        self.bias_grads = scores_grads.sum(0)\n",
        "        return torch.matmul(scores_grads, self.weight.t())\n",
        "\n",
        "# Input: batchSize x numInputs.\n",
        "numInputs = 1 * 28 * 28\n",
        "mock_inputs = torch.Tensor(batchSize, numInputs).normal_(0, 0.1)\n",
        "\n",
        "# Create the linear object to use.\n",
        "linear = toynn_Linear(numInputs, 10)\n",
        "\n",
        "# Forward and Backward passes:\n",
        "scores = linear.forward(mock_inputs)\n",
        "mock_inputs_grads = linear.backward(mock_inputs, mock_scores_grads)\n",
        "\n",
        "print(\"Input x: \", mock_inputs.shape)\n",
        "print(\"Weights W: \", linear.weight.shape)\n",
        "print(\"Biases b: \", linear.bias.shape)\n",
        "print(\"Outputs: \", scores.shape)\n",
        "print(\"dL / dx: \", mock_inputs_grads.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input x:  torch.Size([32, 784])\n",
            "Weights W:  torch.Size([784, 10])\n",
            "Biases b:  torch.Size([10])\n",
            "Outputs:  torch.Size([32, 10])\n",
            "dL / dx:  torch.Size([32, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNiPMoctZ5T_",
        "colab_type": "text"
      },
      "source": [
        "We are finished with a cleaner implementation of the linear + softmax + negative log-likelihood classifier that we implemented for the previous assignment: (1) It supports batches,  (2) the functions for forward and backward are nicely packaged in a python class, (3) the weight and bias matrices (as well as weight_grad and bias_grad matrices) are nicely created and initialized in the constructor of the toynn_Linear class, (4) the Linear class also computes $\\partial \\ell / \\partial x_j$'s, which will be useful to stack layers in order to train deeper models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGdZ-bmxb473",
        "colab_type": "text"
      },
      "source": [
        "### 2. Mini-batch SGD on FashionMNIST\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUfYXo8McA14",
        "colab_type": "text"
      },
      "source": [
        "Given the newly implemented toynn_CrossEntropyLoss and toynn_Linear classes, let's train a classifier which is exactly the same as in the previous assignment, but now we can use batches of examples, as opposed to single examples during training with Stochastic (mini-batch) Gradient Descent (SGD). There are a few changes to make to the code from the previous assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJDmV7Xgb3RI",
        "colab_type": "code",
        "outputId": "bd65a1aa-8939-4e16-a041-aaf014e96cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# Removes, the need to call F.to_image ourselves.\n",
        "# Also, please look up what transforms.Normalize does.\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Load the training, and validation datasets.\n",
        "trainset = FashionMNIST(root = './data', train = True, transform = transform, download = True)\n",
        "valset = FashionMNIST(root = './data', train = False, transform = transform, download = True)\n",
        "\n",
        "# NEW: Pytorch DataLoader for iterating over batches.\n",
        "batchSize = 100\n",
        "\n",
        "# Shuffling is needed in case dataset is not shuffled by default.\n",
        "train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
        "                                           batch_size = batchSize,\n",
        "                                           shuffle = True)\n",
        "# We don't need to bach the validation set but let's do it anyway.\n",
        "val_loader = torch.utils.data.DataLoader(dataset = valset,\n",
        "                                         batch_size = batchSize,\n",
        "                                         shuffle = False) # No need.\n",
        "\n",
        "# Define a learning rate. \n",
        "learningRate = 1e-4\n",
        "\n",
        "# Define number of epochs.\n",
        "N = 5\n",
        "\n",
        "# Create the model.\n",
        "loss_fn = toynn_CrossEntropyLoss()\n",
        "linear_fn = toynn_Linear(1 * 28 * 28, 10)\n",
        "\n",
        "\n",
        "# log accuracies and losses.\n",
        "train_accuracies = []; val_accuracies = []\n",
        "train_losses = []; val_losses = []\n",
        "\n",
        "# Training loop. Please make sure you understand every single line of code below.\n",
        "# Go back to some of the previous steps in this lab if necessary.\n",
        "for epoch in range(0, N):\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    \n",
        "    # Make a pass over the training data.\n",
        "    for (i, (inputs, labels)) in enumerate(train_loader):\n",
        "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        scores = linear_fn.forward(inputs)\n",
        "        cum_loss += loss_fn.forward(scores, labels).item()\n",
        "        \n",
        "        # Count how many correct in this batch.\n",
        "        max_scores, max_labels = scores.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "        \n",
        "        #Backward pass. (Gradient computation stage)\n",
        "        scores_grads = loss_fn.backward(scores, labels)\n",
        "        grad_inputs = linear_fn.backward(inputs, scores_grads)\n",
        "        \n",
        "        # Parameter updates (SGD step).\n",
        "        linear_fn.weight.add_(-learningRate, linear_fn.weight_grads)\n",
        "        linear_fn.bias.add_(-learningRate, linear_fn.bias_grads)\n",
        "        \n",
        "        # Logging the current results on training.\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "                  (epoch, i + 1, cum_loss / (i + 1), correct / (i * batchSize + 1)))\n",
        "    \n",
        "    train_accuracies.append(correct / len(trainset))\n",
        "    train_losses.append(cum_loss / len(trainset))\n",
        "    \n",
        "    \n",
        "    # Make a pass over the validation data.\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    for (i, (inputs, labels)) in enumerate(val_loader):\n",
        "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        scores = linear_fn.forward(inputs)\n",
        "        cum_loss += loss_fn.forward(scores, labels).item()\n",
        "        \n",
        "         # Count how many correct in this batch.\n",
        "        max_scores, max_labels = scores.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "          \n",
        "    val_accuracies.append(correct / len(valset))\n",
        "    val_losses.append(cum_loss / (i + 1))\n",
        "            \n",
        "    # Logging the current results on validation.\n",
        "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "          (epoch, cum_loss / (i + 1), correct / len(valset)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-epoch 0. Iteration 00100, Avg-Loss: 1.6778, Accuracy: 0.5721\n",
            "Train-epoch 0. Iteration 00200, Avg-Loss: 1.4243, Accuracy: 0.6224\n",
            "Train-epoch 0. Iteration 00300, Avg-Loss: 1.2850, Accuracy: 0.6447\n",
            "Train-epoch 0. Iteration 00400, Avg-Loss: 1.1896, Accuracy: 0.6640\n",
            "Train-epoch 0. Iteration 00500, Avg-Loss: 1.1247, Accuracy: 0.6772\n",
            "Train-epoch 0. Iteration 00600, Avg-Loss: 1.0731, Accuracy: 0.6891\n",
            "Validation-epoch 0. Avg-Loss: 0.8119, Accuracy: 0.7363\n",
            "Train-epoch 1. Iteration 00100, Avg-Loss: 0.7808, Accuracy: 0.7654\n",
            "Train-epoch 1. Iteration 00200, Avg-Loss: 0.7705, Accuracy: 0.7632\n",
            "Train-epoch 1. Iteration 00300, Avg-Loss: 0.7601, Accuracy: 0.7651\n",
            "Train-epoch 1. Iteration 00400, Avg-Loss: 0.7520, Accuracy: 0.7664\n",
            "Train-epoch 1. Iteration 00500, Avg-Loss: 0.7422, Accuracy: 0.7686\n",
            "Train-epoch 1. Iteration 00600, Avg-Loss: 0.7326, Accuracy: 0.7708\n",
            "Validation-epoch 1. Avg-Loss: 0.7027, Accuracy: 0.7721\n",
            "Train-epoch 2. Iteration 00100, Avg-Loss: 0.6780, Accuracy: 0.7938\n",
            "Train-epoch 2. Iteration 00200, Avg-Loss: 0.6713, Accuracy: 0.7933\n",
            "Train-epoch 2. Iteration 00300, Avg-Loss: 0.6681, Accuracy: 0.7930\n",
            "Train-epoch 2. Iteration 00400, Avg-Loss: 0.6654, Accuracy: 0.7915\n",
            "Train-epoch 2. Iteration 00500, Avg-Loss: 0.6603, Accuracy: 0.7935\n",
            "Train-epoch 2. Iteration 00600, Avg-Loss: 0.6562, Accuracy: 0.7948\n",
            "Validation-epoch 2. Avg-Loss: 0.6521, Accuracy: 0.7879\n",
            "Train-epoch 3. Iteration 00100, Avg-Loss: 0.6384, Accuracy: 0.8062\n",
            "Train-epoch 3. Iteration 00200, Avg-Loss: 0.6265, Accuracy: 0.8060\n",
            "Train-epoch 3. Iteration 00300, Avg-Loss: 0.6238, Accuracy: 0.8046\n",
            "Train-epoch 3. Iteration 00400, Avg-Loss: 0.6186, Accuracy: 0.8052\n",
            "Train-epoch 3. Iteration 00500, Avg-Loss: 0.6175, Accuracy: 0.8054\n",
            "Train-epoch 3. Iteration 00600, Avg-Loss: 0.6145, Accuracy: 0.8061\n",
            "Validation-epoch 3. Avg-Loss: 0.6195, Accuracy: 0.7952\n",
            "Train-epoch 4. Iteration 00100, Avg-Loss: 0.5972, Accuracy: 0.8177\n",
            "Train-epoch 4. Iteration 00200, Avg-Loss: 0.5954, Accuracy: 0.8161\n",
            "Train-epoch 4. Iteration 00300, Avg-Loss: 0.5935, Accuracy: 0.8126\n",
            "Train-epoch 4. Iteration 00400, Avg-Loss: 0.5907, Accuracy: 0.8122\n",
            "Train-epoch 4. Iteration 00500, Avg-Loss: 0.5884, Accuracy: 0.8135\n",
            "Train-epoch 4. Iteration 00600, Avg-Loss: 0.5871, Accuracy: 0.8133\n",
            "Validation-epoch 4. Avg-Loss: 0.5993, Accuracy: 0.8024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NheOKVEvktSU",
        "colab_type": "text"
      },
      "source": [
        "We achieved an accuracy of 80% a lot faster than in the previous assignment, in fact we reached almost 83% in the same amount of epochs as before, so it seems mini-batching is helping to some extent. Try experimenting with different batch sizes and learning rates. Batch-size and learning rate are two hyper-parameters that are related in the optimization process, and is a current line of active research. For instance, under deep learning models, larger batch sizes do not offer as good generalization as smaller batches, which is bad because it is easier to parallelize code when using larger batch sizes. Also, a common strategy during training is to reduce (decay) the learning rate, as the training progresses to the later epochs, but a recent paper proposed to increase the batch size instead (for more, see: https://arxiv.org/abs/1711.00489 ). Probably for small datasets, keeping the learning rate fixed and batch size fixed will be fine, but as one moves to larger datasets, and deeper models, it becomes crucial to use some more advanced strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL8RK38atNE5",
        "colab_type": "text"
      },
      "source": [
        "### 3. The Rectified Linear Unit (ReLU) Activation Function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2SukZfmtbH_",
        "colab_type": "text"
      },
      "source": [
        "We are close to implementing a neural network, we can accomplish this by stacking a linear operation on top of a Rectified Linear Unit (ReLU) activation, another linear operation, and the softmax + negative log likelihood loss. This is all it takes to create simple neural network with one hidden layer. First, let's implement the ReLU layer as we implemented the linear layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6ZplFBMwAkD",
        "colab_type": "code",
        "outputId": "c1519d27-1ee1-4cc4-c4cd-a1ad0e504ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "class toynn_ReLU(object):\n",
        "  \n",
        "    # Forward operation: f(x_i) = max(0, x_i)\n",
        "    def forward(self, inputs):\n",
        "        outputs = inputs.clone()\n",
        "        outputs[outputs < 0] = 0\n",
        "        return outputs\n",
        "    \n",
        "    # Make sure the backward pass is absolutely clear.\n",
        "    def backward(self, inputs, outputs_grad):\n",
        "        inputs_grad = outputs_grad.clone() # 1 * previous_grads\n",
        "        inputs_grad[inputs < 0] = 0  # or zero.\n",
        "        return inputs_grad\n",
        "      \n",
        "# Let's test it.\n",
        "x = torch.tensor([-2.3, 2.3, 3.1, -1.3, 4.3])\n",
        "relu_fn = toynn_ReLU()\n",
        "\n",
        "print(\"Input x: \", x)\n",
        "print(\"Output: \", relu_fn.forward(x))\n",
        "print(\"Grad x: \", relu_fn.backward(x, torch.ones(5)))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input x:  tensor([-2.3000,  2.3000,  3.1000, -1.3000,  4.3000])\n",
            "Output:  tensor([0.0000, 2.3000, 3.1000, 0.0000, 4.3000])\n",
            "Grad x:  tensor([0., 1., 1., 0., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajJ1z9qb0oqt",
        "colab_type": "text"
      },
      "source": [
        "### 4. Forward Pass in a Two-Layer Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK4M717G0v-8",
        "colab_type": "text"
      },
      "source": [
        "We are going to show here how to perform inference in a two-layer neural network using the operations defined earlier. In the questions for the assignment it is your task to train this network and demonstrate superior accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH4e6WI21DHw",
        "colab_type": "code",
        "outputId": "5797e4af-5cd7-4dec-9c82-362c17c6b4b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.rc('image', cmap = 'gray')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup the input variable x.\n",
        "img, label = trainset[0]\n",
        "x = img.view(1, 1 * 28 * 28)\n",
        "\n",
        "# Setup the number of inputs, hidden neurons, and outputs.\n",
        "nInputs = 1 * 28 * 28\n",
        "nHidden = 512\n",
        "nOutputs = 10\n",
        "\n",
        "# Create the model here.\n",
        "linear_fn1 = toynn_Linear(nInputs, nHidden)\n",
        "relu_fn = toynn_ReLU()\n",
        "linear_fn2 = toynn_Linear(nHidden, nOutputs)\n",
        "\n",
        "# Make predictions.\n",
        "a = linear_fn1.forward(x)\n",
        "z = relu_fn.forward(a)\n",
        "yhat = linear_fn2.forward(z)\n",
        "\n",
        "# Show the prediction scores for each class.\n",
        "# Yes, pytorch tensors already come with a softmax function.\n",
        "# We need it here because we hard-coded the softmax inside \n",
        "# the loss function.\n",
        "print(yhat.softmax(dim = 1)) \n",
        "\n",
        "plt.imshow(img[0]); plt.axis('off'); plt.grid(False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1012, 0.0984, 0.0991, 0.1005, 0.0969, 0.1023, 0.0981, 0.1056, 0.1013,\n",
            "         0.0966]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAKLklEQVR4nO3duU+VWx/F8X0EJwbBiQBqRKNEGoNx\nHhONGu0MJtgaYmPvf2Ch0drO0loL4xR7MEqMgYJGHFGDqEiYBTy3et/KZ/3enCe8rpP7/ZR3ZcMZ\nWPdJ/GXvXSgWiwmAnyV/+wUA+DPKCZiinIApygmYopyAqUoVFgoF/ikXWGTFYrHwp//OkxMwRTkB\nU5QTMEU5AVOUEzBFOQFTlBMwRTkBU5QTMEU5AVOUEzBFOQFTlBMwRTkBU5QTMEU5AVOUEzBFOQFT\nlBMwRTkBU5QTMEU5AVPyaEz8/xUKfzwl8b/yXjxVW1sr8yNHjmRmjx49yvW7o/dWUVGRmc3Pz+f6\n3XlFr10p9TvjyQmYopyAKcoJmKKcgCnKCZiinIApygmYYs5pZskS/f/LhYUFmW/btk3mly5dkvn0\n9HRmNjk5KdfOzMzI/Pnz5zLPM8uM5pDR5xqtz/Pa1PxW4ckJmKKcgCnKCZiinIApygmYopyAKcoJ\nmGLOaSaaiUVzzhMnTsj85MmTMh8aGsrMli9fLtdWVVXJ/NSpUzK/fft2ZjY8PCzXRnsmo88tUlNT\nk5n9/v1brp2amirpd/LkBExRTsAU5QRMUU7AFOUETFFOwBTlBEwx5zTz69evXOv37t0r85aWFpmr\nOWu0J/LJkycy37Vrl8xv3LiRmfX29sq1/f39Mh8YGJD5vn37ZK4+1+7ubrm2p6dH5ll4cgKmKCdg\ninICpignYIpyAqYoJ2CKUcpfoI5hjLY+Rduu9uzZI/Px8XGZV1dXZ2atra1ybZS/ePFC5q9fv87M\n1JatlFI6ePCgzDs6OmQ+Nzcnc/Xao+NGZ2dnZZ6FJydginICpignYIpyAqYoJ2CKcgKmKCdgqqDm\naoVCQQ/d/qWi6+LyiOacz549k3m0JSyi3lt0DV7e7W7qCsHo+MmXL1/KXM1QU4rf25kzZzKzrVu3\nyrUbNmyQebFY/OOHzpMTMEU5AVOUEzBFOQFTlBMwRTkBU5QTMMV+zhJEs8jFNDo6KvOmpiaZT09P\ny1xd81dZqf9coj2Xao6ZUkorV67MzKI559GjR2V+6NAhmUfHfjY0NGRmjx8/lmtLxZMTMEU5AVOU\nEzBFOQFTlBMwRTkBU5QTMMWcs8xUVVXJPJrXRfnU1FRmNjY2Jtd+//5d5tFe02BvsVwbva/oc1tY\nWJC5mrNu2rRJri0VT07AFOUETFFOwBTlBExRTsAU5QRMUU7AFHPOEuSduamZWrQnsrm5WebRXZBR\nrvZzRufSqhlpSinV19fLXM1JoznlsmXLZB7dS1pXVyfzvr6+zCz6zqI7U7Pw5ARMUU7AFOUETFFO\nwBTlBExRTsAUo5QSREdjVlRUyFyNUi5cuCDXNjY2ynxkZETm6vjJlPTWqOrqark22joVjWLUGGdu\nbk6ujY7tjN732rVrZX7r1q3MrL29Xa6NXlsWnpyAKcoJmKKcgCnKCZiinIApygmYopyAqUJwHOHf\nu+vOWDS3mp+fL/ln79+/X+YPHjyQeXTFX54ZbG1trVwbXfEXHZ25dOnSkrKU4hlsdHViRL23mzdv\nyrV37tyRebFY/OMeRJ6cgCnKCZiinIApygmYopyAKcoJmKKcgKlF3c+pjpCM5m3R8ZLR8ZRq/5/a\ns/i/yDPHjDx8+FDmk5OTMo/mnNERkmruHe0Vjb7TFStWyDzas5lnbfSdR699586dmVl0NWKpeHIC\npignYIpyAqYoJ2CKcgKmKCdginICpnLNOfPsDVzMWeFiO3bsmMzPnz8v88OHD2dm0TV60Z7IaI4Z\n7UVV31n02qK/B3UubUp6DhqdFRy9tkj0uU1MTGRmHR0dcu39+/dLek08OQFTlBMwRTkBU5QTMEU5\nAVOUEzBFOQFTtufWrlmzRubNzc0y3759e8lro7lVa2urzGdnZ2Wu9qpG+xKjeyY/f/4s8+j8VzXv\ni+6wjO7frKqqknl3d3dmVlNTI9dGs+doP2e0J1N9bsPDw3JtW1ubzDm3FigzlBMwRTkBU5QTMEU5\nAVOUEzCVa5Ry4MAB+cOvXr2ama1fv16ura+vl7na2pSS3r708+dPuTbazhaNBKKRgjrWMzracmBg\nQOadnZ0y7+3tlbm65m/16tVybUtLi8wjb968ycyi6wfHx8dlHm0pi0ZUapSzatUquTb6e2GUApQZ\nygmYopyAKcoJmKKcgCnKCZiinIApOeesrKyUc86enh75w5uamjKzaE4Z5XmOQoyOcIxmjXnV1dVl\nZuvWrZNrL168KPPTp0/L/PLlyzJXW85mZmbk2rdv38pczTFT0tv88m5Xi7bKRXNUtT7ajrZ582aZ\nM+cEygzlBExRTsAU5QRMUU7AFOUETFFOwJScc3Z1dck55/Xr1+UPHxwczMyiow6jPLpOTolmXmoO\nmVJKHz9+lHl0PKXay6qOzUwppcbGRpmfO3dO5uqavZT0nszoO9m9e3euXL33aI4ZfW7RFX8RtQc3\n+nuK9j1/+PCBOSdQTignYIpyAqYoJ2CKcgKmKCdginICpipV+PXrV7k4mvepPXLRNXnRz45mbmqu\nFZ0z+uPHD5m/f/9e5tFrU/tFoz2T0Zm69+7dk3l/f7/M1ZwzupYxmkVG5wWr6w+j9x3tqYxmkdF6\nNeeMZqjRlZFZeHICpignYIpyAqYoJ2CKcgKmKCdgSo5SPn36JBer7WYppTQ0NJSZVVdXy7XREZHR\nP8t/+/YtMxsZGZFrKyvlxxJuV4v+2V5t24qOaIy2Rqn3nVJKbW1tMp+cnMzMovHW6OiozKPPTb12\nNWZJKR61ROujKwDVVr2xsTG5tr29XeZZeHICpignYIpyAqYoJ2CKcgKmKCdginICpuRA79WrV3Lx\n3bt3Zd7V1ZWZRcdHRtfFRVur1LataA4ZzbyiLULRFYNqu1x09WE0W46uRvzy5UvJPz96bdF8OM93\nlnc7Wp7tainpOeqWLVvk2uHhYZln4ckJmKKcgCnKCZiinIApygmYopyAKcoJmJJXABYKBT1UC5w9\nezYzu3Llilzb0NAg82jfopprRfO6aE4ZzTmjeZ/6+eoIxpTiOWc0w41y9d6itdFrj6j1pc4K/yP6\nzqKjMdV+zr6+Prm2s7NT5sVikSsAgXJCOQFTlBMwRTkBU5QTMEU5AVOUEzAl55wVFRVyqBbNhvI4\nfvy4zK9duyZzNSetq6uTa6OzYaM5aDTnjOasSnQtYzQHjc4iVt/pxMSEXBt9LhH12qP9ltE+1ug7\nffr0qcwHBgYys+7ubrk2wpwTKDOUEzBFOQFTlBMwRTkBU5QTMEU5AVOLup/T1Y4dO2Se927QjRs3\nyvzdu3eZWTTPGxwclDnKD3NOoMxQTsAU5QRMUU7AFOUETFFOwNS/cpQCOGGUApQZygmYopyAKcoJ\nmKKcgCnKCZiinIApygmYopyAKcoJmKKcgCnKCZiinIApygmYopyAKbmfE8Dfw5MTMEU5AVOUEzBF\nOQFTlBMwRTkBU/8A0u/ZrlPqEFgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I1rONJT53Wu",
        "colab_type": "text"
      },
      "source": [
        "Since the weights, and biases in the two linear layers [linear_fn1, linear_fn2] are not trained, the predictions are arbitrary at this point. One of the tasks for this assignment is for you to train the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_8eQuKBi7u0",
        "colab_type": "text"
      },
      "source": [
        "## Assignment Questions [100pts + 30pts (optional extra credit)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpPb-CRh47Ct",
        "colab_type": "text"
      },
      "source": [
        "### 1. Activation Functions (30pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJrvZxV6oDPj",
        "colab_type": "text"
      },
      "source": [
        "Provide code for the following activation functions:\n",
        "\n",
        "$$\\text{Sigmoid(x)} = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1}$$\n",
        "\n",
        "$$\\text{Tanh(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
        "\n",
        "$$ \\text{LeakyReLU}(x) = \\begin{cases} \n",
        "      0.01 x & x < 0 \\\\\n",
        "      x & x \\geq 0 \n",
        "\\end{cases}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ReR6YZC47eW",
        "colab_type": "code",
        "outputId": "2879f93f-6185-4334-eb67-e1d729555a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import numpy as np \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class toynn_toynn_Sigmoid:\n",
        "    def forward(self, x):\n",
        "        outputs = x.clone()\n",
        "        outputs = 1 / (1 + np.exp(-x))\n",
        "        return outputs\n",
        "    \n",
        "    def backward(self, x, output_grads):\n",
        "        inputs_grad = output_grads.clone() # 1 * previous_grads  dA * A * (1 - A)\n",
        "        inputs_grad = inputs_grad * x * (1-x)\n",
        "        return inputs_grad\n",
        "\n",
        "class toynn_Tanh:\n",
        "    def forward(self, x):\n",
        "        outputs = x.clone()\n",
        "        outputs = np.tanh(outputs)                 #(outputs.exp() - outputs.exp()) /(outputs.exp() + outputs.exp())\n",
        "        return outputs\n",
        "    \n",
        "    def backward(self, x, output_grads):\n",
        "        inputs_grad = output_grads.clone() # 1 * previous_grads  dZ = dA * (1 - np.square(A))\n",
        "        inputs_grad = inputs_grad * ( 1 - (x * x) )\n",
        "        return inputs_grad\n",
        "\n",
        "class toynn_LeakyReLU:\n",
        "    def forward(self, x):\n",
        "        outputs = x.clone()\n",
        "        outputs[outputs < 0] = 0.01 * outputs[outputs < 0]\n",
        "        return outputs\n",
        "    \n",
        "    def backward(self, x, output_grads):\n",
        "        inputs_grad = output_grads.clone() # 1 * previous_grads\n",
        "        inputs_grad[x < 0] = 0.01 * inputs_grad[x < 0]\n",
        "        return inputs_grad\n",
        "\n",
        "\n",
        "\n",
        "# Let's test it.\n",
        "x = torch.tensor([-2.3, 2.3, 3.1, -1.3, 4.3])\n",
        "relu_fn = toynn_toynn_Sigmoid()\n",
        "A_sigmoid = relu_fn.forward(x)\n",
        "m = nn.Sigmoid()\n",
        "print(\"Input x: \", x)\n",
        "print(\"Output: \", relu_fn.forward(x))\n",
        "print(\"Output: \", m(x))\n",
        "print(\"Output: \", m(x).backward)\n",
        "print(\"Grad x: \", relu_fn.backward(relu_fn.forward(x), torch.ones(5)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input x:  tensor([-2.3000,  2.3000,  3.1000, -1.3000,  4.3000])\n",
            "Output:  tensor([0.0911, 0.9089, 0.9569, 0.2142, 0.9866])\n",
            "Output:  tensor([0.0911, 0.9089, 0.9569, 0.2142, 0.9866])\n",
            "Output:  <bound method Tensor.backward of tensor([0.0911, 0.9089, 0.9569, 0.2142, 0.9866])>\n",
            "Grad x:  tensor([0.0828, 0.0828, 0.0412, 0.1683, 0.0132])\n",
            "Input x:  tensor([-2.3000,  2.3000,  3.1000, -1.3000,  4.3000])\n",
            "Output:  tensor([-0.9801,  0.9801,  0.9959, -0.8617,  0.9996])\n",
            "Output:  tensor([-0.9801,  0.9801,  0.9959, -0.8617,  0.9996])\n",
            "Grad x:  tensor([0.0394, 0.0394, 0.0081, 0.2574, 0.0007])\n",
            "Input x:  tensor([-2.3000,  2.3000,  3.1000, -1.3000,  4.3000])\n",
            "Output:  tensor([-0.0230,  2.3000,  3.1000, -0.0130,  4.3000])\n",
            "Output:  tensor([-0.0230,  2.3000,  3.1000, -0.0130,  4.3000])\n",
            "Grad x:  tensor([0.0100, 1.0000, 1.0000, 0.0100, 1.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVk62dpw1BzF",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAMnCGwv-zJN",
        "colab_type": "text"
      },
      "source": [
        "### 2. Binary Cross Entropy (BCE) loss function: (20pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0BccWLcn7Q1",
        "colab_type": "text"
      },
      "source": [
        "Provide code for the binary cross entropy loss function as defined below:\n",
        "\n",
        "$$\\ell(y, \\hat{y}) = -\\sum_{i=1}^{i=n} [y_i\\text{log}(\\hat{y}_i) + (1 - y_i)\\text{log}(1 - \\hat{y}_i)]$$\n",
        "\n",
        "where $n$ is the number of outputs (e.g. the size of vectors $y$ and $\\hat{y}$), the entries in the target vector $y_i$ are binary $\\in \\{0,1\\}$ and $\\hat{y}_i$ are typically the outputs of a sigmoid layer. Remember that the backward pass does not return a scalar but a vector containing the values for $\\partial \\ell / \\partial \\hat{y}_i$, henceforth a vector of the same size as $\\hat{y}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVC2BJpa-5Ka",
        "colab_type": "code",
        "outputId": "2ab0f674-710a-4d13-d183-d80f121cf7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Binary cross entropy loss. \n",
        "# Useful for classification when the classes are not mutually exclusive.\n",
        "# For instance, when both shoe, and dress, are correct labels for an image.\n",
        "# In other words, when images have multiple labels per image.\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class toynn_BCELoss:\n",
        "    def forward(self, predictions, targets):\n",
        "      #Loss = -(Y Log Yh + (1-Y) Log (1-Yh))\n",
        "      #loss=0\n",
        "      #print(targets.size(0))\n",
        "      #for i in range(0, targets.size(0)):\n",
        "        #k=predictions[i]\n",
        "        #if(targets[i].item()==1):\n",
        "          #loss=loss+k.log()\n",
        "        #else:\n",
        "          #loss=loss+(1-k).log()\n",
        "      #print(-1*(loss))\n",
        "      loss=targets*predictions.log()+(1-targets)*(1-predictions).log()\n",
        "      return -1*(loss.sum())\n",
        "        \n",
        "    def backward(self, predictions, targets):\n",
        "        # Backward pass.\n",
        "        return ((-targets/predictions)+((1-targets)/(1-predictions)))  #dLoss_Yh = — (Y/Yh — (1-Y)/(1-Yh))\n",
        "\n",
        "\n",
        "m = nn.Sigmoid()\n",
        "loss = nn.BCELoss()\n",
        "input = torch.randn(2, requires_grad=True)\n",
        "target = torch.tensor([1., 1.])\n",
        "print(soneya.forward(m(input),target))\n",
        "print(soneya.backward(m(input),target))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.0901, grad_fn=<MulBackward0>)\n",
            "tensor([-2.1483, -3.7640], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGIxg3q5i_bl",
        "colab_type": "text"
      },
      "source": [
        "### 3. Training of the Two Layer NN (50pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5SiFBConq67",
        "colab_type": "text"
      },
      "source": [
        "Train the two-layer Neural Network as defined in Section 3 of this Assignment on FashionMNIST using toynn. Include below the code for training this neural network and report the training, validation plots for loss and accuracy. The code should be similar to the code in Section 2 of this assignment, please follow that convention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzQc9ZtKjnf0",
        "colab_type": "code",
        "outputId": "fdce2207-edaf-4b79-ff77-868082d661c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# Removes, the need to call F.to_image ourselves.\n",
        "# Also, please look up what transforms.Normalize does.\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Load the training, and validation datasets.\n",
        "trainset = FashionMNIST(root = './data', train = True, transform = transform, download = True)\n",
        "valset = FashionMNIST(root = './data', train = False, transform = transform, download = True)\n",
        "\n",
        "# NEW: Pytorch DataLoader for iterating over batches.\n",
        "batchSize = 100\n",
        "\n",
        "# Shuffling is needed in case dataset is not shuffled by default.\n",
        "train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
        "                                           batch_size = batchSize,\n",
        "                                           shuffle = True)\n",
        "# We don't need to bach the validation set but let's do it anyway.\n",
        "val_loader = torch.utils.data.DataLoader(dataset = valset,batch_size = batchSize, shuffle = False) # No need.\n",
        "\n",
        "# Define a learning rate. \n",
        "learningRate = 1e-3\n",
        "\n",
        "# Define number of epochs.\n",
        "N = 4\n",
        "# Setup the number of inputs, hidden neurons, and outputs.\n",
        "nInputs = 1 * 28 * 28\n",
        "nHidden = 512\n",
        "nOutputs = 10\n",
        "\n",
        "# Create the model here.\n",
        "linear_fn1 = toynn_Linear(nInputs, nHidden)\n",
        "relu_fn = toynn_ReLU()\n",
        "linear_fn2 = toynn_Linear(nHidden, nOutputs)\n",
        "loss_fn = toynn_CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# log accuracies and losses.\n",
        "train_accuracies = []; val_accuracies = []\n",
        "train_losses = []; val_losses = []\n",
        "\n",
        "# Training loop. Please make sure you understand every single line of code below.\n",
        "# Go back to some of the previous steps in this lab if necessary.\n",
        "for epoch in range(0, N):\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    \n",
        "    # Make a pass over the training data.\n",
        "    for (i, (inputs, labels)) in enumerate(train_loader):\n",
        "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "\n",
        "        # Make predictions.\n",
        "        a = linear_fn1.forward(inputs)\n",
        "        z = relu_fn.forward(a)\n",
        "        scores = linear_fn2.forward(z)\n",
        "        cum_loss += loss_fn.forward(scores, labels).item()\n",
        "        \n",
        "        # Count how many correct in this batch.\n",
        "        max_scores, max_labels = scores.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "        \n",
        "        #Backward pass. (Gradient computation stage)\n",
        "        scores_grads = loss_fn.backward(scores, labels)\n",
        "        linear2_grad = linear_fn2.backward(z, scores_grads)\n",
        "        relu_grad = relu_fn.backward(a, linear2_grad)\n",
        "        grad_inputs = linear_fn1.backward(inputs, relu_grad)\n",
        "        \n",
        "        # Parameter updates (SGD step).\n",
        "        linear_fn1.weight.add_(-learningRate, linear_fn1.weight_grads)\n",
        "        linear_fn1.bias.add_(-learningRate, linear_fn1.bias_grads)\n",
        "        linear_fn2.weight.add_(-learningRate, linear_fn2.weight_grads)\n",
        "        linear_fn2.bias.add_(-learningRate, linear_fn2.bias_grads)\n",
        "        \n",
        "        \n",
        "        # Logging the current results on training.\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "                  (epoch, i + 1, cum_loss / (i + 1), correct / (i * batchSize + 1)))\n",
        "    \n",
        "    train_accuracies.append(correct / len(trainset))\n",
        "    train_losses.append(cum_loss / len(trainset))\n",
        "    \n",
        "    \n",
        "    # Make a pass over the validation data.\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    for (i, (inputs, labels)) in enumerate(val_loader):\n",
        "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        # Make predictions.\n",
        "        a = linear_fn1.forward(inputs)\n",
        "        z = relu_fn.forward(a)\n",
        "        scores = linear_fn2.forward(z)\n",
        "        cum_loss += loss_fn.forward(scores, labels).item()\n",
        "\n",
        "        # Count how many correct in this batch.\n",
        "        max_scores, max_labels = scores.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "          \n",
        "    val_accuracies.append(correct / len(valset))\n",
        "    val_losses.append(cum_loss / (i + 1))\n",
        "            \n",
        "    # Logging the current results on validation.\n",
        "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "          (epoch, cum_loss / (i + 1), correct / len(valset)))\n",
        "    \n",
        "\n",
        "plt.figure(figsize = (10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(val_losses, 'bo-', label = 'val-loss')\n",
        "plt.plot(train_losses, 'ro-', label = 'train-loss')\n",
        "plt.grid('on')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['validation', 'training'], loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, 'bo-', label = 'val-acc')\n",
        "plt.plot(train_accuracies, 'ro-', label = 'train-acc')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid('on')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['validation', 'training'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-epoch 0. Iteration 00100, Avg-Loss: 1.3523, Accuracy: 0.5524\n",
            "Train-epoch 0. Iteration 00200, Avg-Loss: 1.0641, Accuracy: 0.6321\n",
            "Train-epoch 0. Iteration 00300, Avg-Loss: 0.9270, Accuracy: 0.6787\n",
            "Train-epoch 0. Iteration 00400, Avg-Loss: 0.8432, Accuracy: 0.7084\n",
            "Train-epoch 0. Iteration 00500, Avg-Loss: 0.7853, Accuracy: 0.7281\n",
            "Train-epoch 0. Iteration 00600, Avg-Loss: 0.7448, Accuracy: 0.7416\n",
            "Validation-epoch 0. Avg-Loss: 0.5277, Accuracy: 0.8144\n",
            "Train-epoch 1. Iteration 00100, Avg-Loss: 0.5116, Accuracy: 0.8240\n",
            "Train-epoch 1. Iteration 00200, Avg-Loss: 0.4983, Accuracy: 0.8297\n",
            "Train-epoch 1. Iteration 00300, Avg-Loss: 0.4927, Accuracy: 0.8309\n",
            "Train-epoch 1. Iteration 00400, Avg-Loss: 0.4895, Accuracy: 0.8312\n",
            "Train-epoch 1. Iteration 00500, Avg-Loss: 0.4828, Accuracy: 0.8315\n",
            "Train-epoch 1. Iteration 00600, Avg-Loss: 0.4767, Accuracy: 0.8335\n",
            "Validation-epoch 1. Avg-Loss: 0.4669, Accuracy: 0.8322\n",
            "Train-epoch 2. Iteration 00100, Avg-Loss: 0.4430, Accuracy: 0.8496\n",
            "Train-epoch 2. Iteration 00200, Avg-Loss: 0.4386, Accuracy: 0.8484\n",
            "Train-epoch 2. Iteration 00300, Avg-Loss: 0.4346, Accuracy: 0.8491\n",
            "Train-epoch 2. Iteration 00400, Avg-Loss: 0.4308, Accuracy: 0.8496\n",
            "Train-epoch 2. Iteration 00500, Avg-Loss: 0.4266, Accuracy: 0.8506\n",
            "Train-epoch 2. Iteration 00600, Avg-Loss: 0.4236, Accuracy: 0.8510\n",
            "Validation-epoch 2. Avg-Loss: 0.4430, Accuracy: 0.8398\n",
            "Train-epoch 3. Iteration 00100, Avg-Loss: 0.3948, Accuracy: 0.8661\n",
            "Train-epoch 3. Iteration 00200, Avg-Loss: 0.3961, Accuracy: 0.8626\n",
            "Train-epoch 3. Iteration 00300, Avg-Loss: 0.3906, Accuracy: 0.8633\n",
            "Train-epoch 3. Iteration 00400, Avg-Loss: 0.3920, Accuracy: 0.8627\n",
            "Train-epoch 3. Iteration 00500, Avg-Loss: 0.3927, Accuracy: 0.8619\n",
            "Train-epoch 3. Iteration 00600, Avg-Loss: 0.3934, Accuracy: 0.8618\n",
            "Validation-epoch 3. Avg-Loss: 0.4095, Accuracy: 0.8519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAEGCAYAAAAt7EI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeZzN9f7A8dfbWMaM9VJkLDOtlohI\nCRlSadWikG6UrtuqtPzSdbOVrnvbu6kuQrdIUkql6F4mKbpoEUORZYw9IYzBjPfvj88cc2acYZaz\nfOfM+/l4fB9zvt/v53vOe46Z4z2fz+f7/oiqYowxxhhjwqtcpAMwxhhjjCmLLAkzxhhjjIkAS8KM\nMcYYYyLAkjBjjDHGmAiwJMwYY4wxJgLKRzqAoqpdu7YmJiYWuv3+/fuJj48PXUDFZHEVnVdj82pc\n4N3YihrX0qVLf1XVk0IYUljY51foeTU2r8YF3o3Nq3FB0WI77ueXqpaqrXXr1loU8+bNK1L7cLG4\nis6rsXk1LlXvxlbUuIAl6oHPn5Ju9vkVel6NzatxqXo3Nq/GpVq02I73+WXDkcYYY4wxEWBJmDHG\nGGNMBFgSZowxxhgTAaVuYr4xXnH48GHS09OpXr06K1eujHQ4AXk1toLiio2NpX79+lSoUCECURlj\nTHhZEmZMMaWnp1O1alVq1apFtWrVIh1OQHv37qVq1aqRDuMYgeJSVXbu3El6ejpJSUkRiswYY8In\naocjJ0+GxETo0qUTiYlu35hgyszMpFatWohIpEOJCiJCrVq1yMzMjHQoxhgTWE5y0alLF4KRXERl\nEjZ5MgwYABs2gKqwYYPbt0TMBJslYMEVifdTRLqJyE8iskZEBgc431BE5onIdyKyTESu8DvXQkQW\nisgKEflRRGLDG70xJmz8kgtRJRjJRVQmYUOGQEZG3mMZGe64Mcb4iEgMMAa4HGgK9BaRpvma/RWY\npqqtgF7AKznXlgfeAu5U1WZAMnA4TKEbY8Jt8OCgJxdRmYSlpQU+vmEDbNsW3liM8ZIqVaoAsHnz\nZnr06BGwTXJyMkuWLDnu87zwwgtk+H0YXXHFFezevTt4gYZPW2CNqq5V1UPAVKB7vjYK+Cb9VQc2\n5zy+FFimqj8AqOpOVc0OQ8zGmFBThXXr4K234M47oXlzSE8P3LagpKMQonJifsOGLuEKpG5daNsW\nrr4arroKzjkHbETJhMPkye4PprQ09zM6ahT06ROZWOrVq8f06dOLff0LL7zALbfcQlxcHACzZs0K\nVmjhlgBs9NtPB87P12Y4MEdE7gPiga45x88EVERmAycBU1X1H/lfQEQGAAMA6tSpQ0pKSqGD27dv\nX5Hah4tX4wLvxubVuMC7sYUzLsnOpsqaNVT78UeqL19O9eXLqbRzJwBZ8fH83rQpVePjqbB//zHX\nZp58MouKG2dBpfS9uhVm2Y+33lKNi1N1qazb4uJUR41SfeIJ1bZtc4/Xr696552qn3yimpFR2EUI\nSs6ryzF4NS5V78WWmpqqqqq///77CdsW9DP51lsli+HRRx/Vl19++ej+sGHD9IknntAuXbpoq1at\ntGnTpvrBBx8cPR8fH6+qquvWrdNmzZqpqmpGRob27NlTGzdurNdee622bdtWFy9erKqqd955p7Zu\n3VqbNm2qQ4cOVVXVF198UStUqKBnn322Jicnq6pqo0aNdMeOHaqq+uyzz2qzZs20WbNm+vzzzx99\nvcaNG+sdd9yhTZs21c6dO2tGAb9wvvfVHyFatgjoAYz32/8j8HK+Ng8CD+U8bgek4kYRHgbWAbWB\nOGAhcPHxXs+WLQo9r8bm1bhUvRtbSOPas0d19mzVxx9X7dJFNT4+98O5YUPVm29WHTNG9fvvVbOy\n3DXF/CA/3udXVPaE+XoXXK+D0rCh5Ol1+OtfYetWmDULPv4Y3nwTXnsNKleGrl1dL9mVV0K9epH7\nHkzp8sAD8P33BZ9ftAgOHsx7LCMD+veHceMCX9OyJbzwwvFft2fPnjzwwAPcc889AEybNo3Zs2cz\ncOBAqlWrxvr16+natSvXXHNNgZPeX331VeLi4li5ciXLli3j3HPPPXpu1KhR/OEPfyA7O5uLL76Y\nZcuWMXDgQJ577jnmzZtH7dq18zzX0qVLmThxIt988w2qyvnnn0+nTp2oWbMmq1ev5u2332bcuHFc\nf/31vPfee9xyyy3H/wZDbxPQwG+/fs4xf/2BbgCqujBn8n1tXK/ZfFX9FUBEZgHnAv8NddDGmCLa\nuBEWLICvvnLbsmVw5AiUK+eGxG67DTp0gPbtoX79wM/hl1xoWhoShCGNqEzCwL0nffpASsoXJCcn\nH3O+bl24/Xa3HTwIKSkuIfvoI7cBtG7thiyvugrOPdf9WxlTHPkTsBMdL6xWrVqxfft2Nm/ezI4d\nO6hZsyZ169Zl0KBBzJ8/H4BNmzaxbds26tatG/A55s+fz8CBAwFo0aIFLVq0OHpu2rRpjB07lqys\nLLZs2UJqamqe8/ktWLCA6667jvj4eACuv/56vvzyS6655hqSkpJo2bIlAC1btmT9+vUl++aDYzFw\nhogk4ZKvXsDN+dqkARcDk0SkCRAL7ABmA/8nInHAIaAT8Hy4AjfGFCA7G3780SVbvsRrY86sg/h4\naNcOHn/cJVwXXABFqaWYk1x8kZISMLcoqqhNwoqiUiW47DK3vfQSrFjhErGPP4aRI2HECDjlFNc7\ndvXVcPHF7t/RGJ8T9VglJgaep9iokfsDoCRuvPFGpk+fztatW+nZsyeTJ09mx44dLF26lMzMTJo3\nb16s2lvr1q3jmWeeYfHixdSsWZN+/fqVqIZXpUqVjj6OiYnh8OHI30ioqlkici8uoYoBJqjqChEZ\niRtCmAk8BIwTkUG4Sfr9coYYdonIc7hEToFZqvpJZL4TY8qw/fvhm29yE66FC2HvXncuIcElW488\n4r62aAHlvZP6eCcSjxCBs89222OPwY4d8OmnLiF75x0YP94lbRdfnNtL1qDBiZ/XlG2jRrlyMv53\nN8fFueMl1bNnT/70pz/x66+/8sUXXzBt2jROPvlkKlSowJw5c9hQ0F0qOS666CKmTJlCly5dWL58\nOcuWLQPg999/Jz4+nurVq7Nt2zY+/fTTo3/5Va1alb179x4zHNmxY0f69evH4MGDUVVmzJjBm2++\nWfJvMoRUdRYwK9+xoX6PU4H2BVz7Fq5MhTEmXDZvzh1WXLDAzQXJznb/gTdvDrfc4hKuDh3cXVAe\nvvvOkrATOOkkuPVWtx06BF9+mTtkOWsW3H23G06+6irXS3beeTZsaY6Vd55icO+ObNasGXv37iUh\nIYFTTjmFPn36cPXVV9O8eXPOOeccGjdufNzr77rrLm677TaaNGlCkyZNaN26NQDnnHMOrVq1onHj\nxjRo0ID27XPzkAEDBtCtWzfq1avHvHnzjh4/99xz6devH23btgXgjjvuoFWrVl4ZejTGlDZHjkBq\nat75XOvWuXOVK8P557v6XR06uKHFGjUiG29RFTRjPxgbbjLrT8AaYHCA8/1wcyu+z9nuONFzeuXu\noiNHVFeuVP3HP1Qvuki1XDl3o8TJJ6vedpvqe++pHu+muTJ5N0oJeS22otwdGSleje14cYXz7shw\nb175/Copr8al6t3YvBqXqsdi279fNSVFddQo/fX881Vr1Mi9E7FOHdUbblB97jnVb75RPXQoYmEW\n5T073udXyHrC/CpRX4K7i2ixiMxU17Xv7x1VvTdUcYSKCDRu7LZHHoHffoPPPnPDljNmwMSJULEi\nJCfn9pIlJkY6amOMMcZDtm3L7eH66itYuhSysgCIbdQIbrwx967FU0/19NBicYRyOPJoJWoAEfFV\nos6fhEWFP/wBbr7ZbYcPw9df507uHzjQbc2a5RaJzba62sYYY8oSVVi1Ku9di2vWuHOVKrlK6g8/\n7BKuCy9k8bJlQbkD0ctCmYQVphI1wA0ichHwMzBIVTfmb1BaK077Ju6np1dm4cJaLFxYi6efrs7o\n0eWoWvVCLrhgK+3a7eS8836jShVvZGVerZwM3outevXq7N27l+zsbPb67sTxGK/Gdry4MjMzPfXv\nbIwppsxM17PlP5/rt9/cudq1XbL15z+7r+ee6xIxj8td+aRTUOb2Rnpi/kfA26p6UET+DLwBdMnf\nSFXHAmMB2rRpo0XJjFOCVMujpHw1KXfvhjlzYPz431i6tC6ff16X8uXhootye8lOPz1ycXrl/QrE\na7GtXLny6F2CVYtSZyaMvBrb8eKKjY2lVatWYY7IGFNiv/7qhoF8PV1Llrg72gDOPBOuvdYlXO3b\nu/1SNrQ4ebL/Xe7Chg1uH4qfiIUyCTthJWpV3em3Ox44Zt21aFOjBtx0E5x88io6dqzLwoVuyPLj\nj2HQILeddVZuQta+vadKmhhjjDFuaHHNmry9XKtWuXMVKkCbNm4eTocOcOGFrtRAKTd4cN4yQ+D2\nhwzxZhJ2wkrUInKKqm7J2b0GWBnCeDwnJsb9fHboAKNHw9q18Mknbi7Ziy/CM8+4pO3yy11Cdvnl\nULNmpKM2xhhT5hw6BN9+m3cS/fbt7lzNmq7HoG9f97VNG1c+opTbv9/lmP/9L8ydC+npgdulpRX/\nNUKWhGnhKlEPFJFrgCzgN1zJijLr1FPhvvvctnevG7b8+GOXmL39tkva2rfP7SU766xS15trgmz3\n7t1MmTKFu+++u0jXXXHFFUyZMoUax6mpM3ToUC666CK6du1a0jCNMV6UM8GpU6Dihbt25Q4tfvUV\n/O9/bo4XwGmnQbduuXctNm4cFQUyDx1y6/zOneu2RYvcjXYVK7qVjqpXhz17jr2uYcPiv2ZIB7r0\nxJWoHwMeC2UMpVXVqnDDDW47csT9/PvWtnzkEbedfnru5P+OHd0PivGw3BmdQavWunv3bl555ZVj\nkrCsrCzKH2cce9asWQWe8xk5cmSJYjPGeJjfBCcBt67a7bfDhAmubMSKFa5d+fJu0vxdd+XO5ypg\nHdrSJjsbvvsut6drwQI3vFiunFs7+sEH3eo47du7FU7yzglzSrryic02KgXKlXOFgC+4AJ580v0f\n7ptH9uqrbt3CatXc2pdXX+2GLfOtJmMiLf9vbzBmdAKDBw/ml19+oWXLllSoUIHY2Fhq1qzJqlWr\n+Pnnn+nduzdbtmwhMzOT+++/nwE5r5mYmMiSJUvYt28fl19+OR06dODrr78mISGBDz/8kMqVK9Ov\nXz+uuuoqevToQWJiIn379uWjjz7i8OHDvPvuuzRu3JgdO3Zw8803s3nzZtq1a8fnn3/O0qVLj1nO\nyBjjMX/5y7ETnA4dgnnz3H8mvXu77KNtW5dpRAFVV3x/7lyXeKWk5PZsNWsGd9wBXbpAp06BC+/n\nXflEadhQSv3dkaYYGjZ0yyXdfbcbs/7Pf3KTsnffdUlbu3a5vWTNmtmwZcg98IBbv6wgixbBwYN5\nj2VkQP/+MG5c4GtatjzhyuCjR49m+fLlfP/996SkpHDllVeyfPlykpKSABgzZgyNGjXiwIEDnHfe\nedxwww3UqlUrz3OsXr2at99+m3HjxnHTTTfx3nvvcYvvdl4/tWvX5ttvv+WVV17hmWeeYfz48YwY\nMYIuXbrw2GOP8dlnn/H6668fN15jTIT9+CNMmnT8iUyffhq2cEJt3brcnq65c10nH7jpPzfe6Hq6\nOneGOnUK93x9+rgtJeWLoNytb0lYKRcfD927u+3IETdv0lck9rHH3JaYmFu1v1OnUlGKJfrkT8BO\ndLyY2rZtezQBA3jttdeODj1u3LiR1atXH5OEJSUl0bJlSwBat25d4DqP119//dE277//PgALFixg\nxowZAHTr1o2adueIMd7z669uYvGkSe4/iQoV3MT5AweObVuSCU4esGWL68zz9Xb5Ps7q1oWuXV1P\nV5cu3lnBxpKwKFKunLsppU0bGDECNm1yk/o//hhefx1efhmqVIFLL3VJ2RVX5Gb/wS5AV+acoMeK\nxEQ3BJlfo0auTzxI4uPjjz5OSUkhJSWFhQsXEhcXR3JyMpm+ibV+Kvll5TExMRwI9MHs1y4mJoas\nnGVFjDEedfiw69GaNMn9J3D4sJvb9dJLbqhx9uzgT3CKgF274Isvcnu7UnPW5KlRw/VwPfSQ6+1q\n3NibI0KWhEWxhAT3O+b7PZs3L7eX7P333Q9k27bQoIE75v5/Dk4BOpPPqFEh+cDzFYsNZM+ePdSo\nUYO4uDhWrVrFokWLSvRagbRv355p06bx6KOPMmfOHHbt2hX01zDGFMGyZS7xeust2LEDTj7Z3XLf\nty+0aJHbzm+Ck6alIaXkr29f2QhfT9e337q5XnFx7ga1fv1cT1fLlq6igNdZElZGxMXBlVe6TRV+\n+CE3IZs+/dj2GRluCa8uXVxvWRTcfRxZeWd0Bu3uyFq1atG+fXvOPvtsKleuTB2/iQ3dunXj5Zdf\npkmTJpx11llccMEFJXqtQIYNG0bv3r158803adeuHXXr1vVkhX5jotqOHTBlCrzxhrvdr0IFuOYa\nl5FcdpnbDyRngtMXHluNxN+hQ/DNN7k9Xb6yERUquLnPw4a5nq62bUtnhQBLwsogEfdXQsuW8Pjj\nLsFSPbbd1q1Qr577YW/QwOUNjRq5r/5bgwZubpo5Ad+MziCbMmVKwOOVKlXi/fffD5gU+eZ91a5d\nm+XLlx89/vDDDx99PGnSpGPaA7Rp0+bo2o7Vq1dn9uzZlC9fnoULF7J48eI8w5vGmBA5fBhmzcod\nbszKcnUV/vlPN9yYb+5naeErGzF3Lrz7bgtSU3PLRpx7risb0aWLu3EzGv7fsSTM0LBh4OlKJ50E\nw4e7jhvfNneum2t25EjetrVqHZuc+W9161pvWjRKS0vjpptu4siRI1SsWJFxBd3paYwJjh9+cInX\n5MmuB6xOHbj/fjfc2Lx5pKMrMlVYuTK3pyslxa2xDJCYWIn+/V1P10UXReeKMZaEmQKnKz3/fOCO\nm6ws2Lw5b3Lm29auzVt7xadCBahfP3BPmq83rUqVkH6bJgTOOOMMvvvuu0iHYUx0277dDTdOmuSS\nsIoV8w43lrIFhtety53T5V82IikJevRwPV2dO8OqVYs9O0waLKXrX86ERFEL0JUvn5s8FWTPnsBJ\nWlqaS9I2bXLdzv7+8Ifj96blb+8FGmgc1xSbvZ/G5Dh0KHe48ZNP3F+/553nbnPv1atUDTdu3Zpb\np2vuXJeEgRshufji3LIRftV1gNz1wKOZJWEGCH4BuurVXc94Qb3jWVmunkugJG39epg/P7dL2qd8\n+YuOzk0raAtnb1psbCw7d+6kYmmcDepBqsrOnTuJjY2NdCjGRIaqK/o8aZLr+fr1V5epDBrkhhub\nNYt0hIXiKxvh6+3yLxuRnJw7r6tJE2+WjQgnS8JMRJQv74YgGzRwEywD+f33vMnZl19uRKQRaWku\nSUtPP7Z3rGbN4ydpp5wSvNuW69evT3p6Ort37/Zs4pCZmenJ2AqKKzY2lvr160cgImMiaPt2N8dr\n0iRXYqJiRVeBu18/V9jR48ON/mUj5s51ZSOOHMktG9G3r+vxKi1lI8LJ2/+ypkyrVg3OPtttAI0b\nryM5udHR89nZBfemuaQtUG+aq59W0Ny0hg3d4umFMW1aBYYMSQraGmKhkJKSQqtWrSIdxjG8Gpcx\nYXPokBtmnDTJDTtmZbk6C6+8Aj17uvkZHnW8shEXXABDh7qervPPL51lI8LJkjBTasXEuMn+9evD\nhRcGbvP777BxY+AkbcEC15uWv/h7jRon7k175x3/mxmswK0xphBUXf0F33Djzp3uA+XBB113UdOm\nkY4wIP+yEXPnuj9wMzLcUGLr1m60tEsX6NAhOspGhJMlYSaqVavmplEUNJUiO9tNGi2oN+3rr+G3\n3/Je4+tOzz8UmpHh1uq0JKx0EZFuwItADDBeVUfnO98QeAOokdNmsKrOync+FRiuqs+ELXBTemzb\nRv1p02DgQLeAdqVKcO21LvG65JKIDTcWtFydr2yEb06Xf9mIpk2hf3+XdHXqFJ1lI8LJkjBTpsXE\nuOHJhARXfTmQvXuP7U0raLWhjRvhtNPcB5X/1qSJleDwIhGJAcYAlwDpwGIRmamqqX7N/gpMU9VX\nRaQpMAtI9Dv/HPBpmEI2pcXBg66I6qRJ8OmnnJ6d7cbnXn3VDTdGOHuZPPnY3vzbb4cxY9zdi1u3\nunZJSXDDDW5OV+fO7j4BEzyWhBlzAlWr5iZTPm+9FbjAbfXq7i7y1FSYM8fNnfBp1Chwcla9eui/\nB1OgtsAaVV0LICJTge64ni0fBarlPK4ObPadEJFrgXXA/rBEa7xN1c1K9w03/vabW3bkkUf4X5Mm\ntL311khHyJEj7o/FBx/MWxsScud69epVcNkIE1yWhBlTDAUVuB0zJnc4MivLFa9NTc27zZvnWyzd\nSUg4Njlr2tTT83KjSQKw0W8/HTg/X5vhwBwRuQ+IB7oCiEgV4FFcL9rDFEBEBgADAOrUqXN0yafC\n2LdvX5Hah4tX44LIxFbxt9+o8/nn1P3sM+LXr+dIhQrs6NiRrZddxq7WrSEmJuxxuRuXKrN+fRwb\nNsSzYYP7mpYWR2Zmwbcoqip/+tMXgPtDM9Afm+FSFn7OLAkzphgKU+C2fHk480y3XXtt7vHsbFcL\nLX9yNn68u9Xbp04dN5ctf3J20klh+RZNrt7AJFV9VkTaAW+KyNm45Ox5Vd0nxyl2pKpjgbEAbdq0\n0aLU4Uvx6MLKXo0LwhjbwYPw0Ueu1+uzz9wvdrt2MHgw5Xr2pE6NGtQJQ1yHDsHq1e4zZOXK3M+T\nn392Ifr4/ti7/HLXAz9sWG6len8NG4pn/m3Lws+ZJWHGFFNxC9zGxLh5Y6edBldfnXvcN0yQPzl7\n4w03L82ndu3APWd161rhw2LYBDTw26+fc8xff6AbgKouFJFYoDaux6yHiPwDN2n/iIhkqurLoQ/b\nRIQqLFnifimnTHFVSRMS4P/+z02yP+uskL10Rgb89FPeRGvlSpeA+W4SEoHERPd50K2bS7aaNoXG\njY+d9lClSuDe/ILmu5rQsCTMGI8oV87NG2vUyP216qPqlnnKn5xNnZq3DlqNGu4D17/3bM+eSqha\ncnYci4EzRCQJl3z1Am7O1yYNuBiYJCJNgFhgh6p29DUQkeHAPkvAotSWLW4i6KRJ7pcvNhauu84V\nU7344qBWIP39d7dcj3+ilZrqJsv7VvWKiYHTT3e/49dfn/v7ftZZLpEqjKIuV2dCw5IwYzxOJLce\n2qWX5h5XdcMJ+ZOzGTNg3Dhfq3Z5bizw3xo2dIlfWaaqWSJyLzAbV35igqquEJGRwBJVnQk8BIwT\nkUG4Sfr91Ba5jH6ZmTBzpuv1+uwz11V94YUwdizcdFOJ76jZufPYXq3UVFe70KdiRZdYtWkDt96a\n+7t7+umuykVJBXu5OlN0loQZU0qJuCHIunXdXUz+duxwH+gffPAzWVlnkpoKn34KEyfmtomLyx2u\n8O89S0wsW0uL5NT8mpXv2FC/x6lAAYtrHW0zPCTBmfBShcWLXY/X22+7rub69WHwYDfceOaZRX46\n3x9KH36YwLvv5iZd27fntvP9LiYn571z+tRTPb9ikSkh++c1JgqddJIrpKi6meTk3P84fvst71/f\nK1a4goxvvpl7bWysm0OSv+fstNPsPwQTpTZvzh1uXLnS/RLccIMbbuzc+YR/lajmzuf0//1KTfWf\nMnAG1au75Oqqq3ITLeuVLtvsI9WYMuQPf3ALpudfNH3PnmP/8/jqKzf32KdiRdcR4EvKfL1np59u\n68OZUigzEz780CVec+a44cb27d1Y/o03BhxuzM52c7Py/66sWgX79uW2890806tXbrK1e/fX3HDD\nhTY/0+RhSZgxhurV3cK7F1yQ9/i+fXknCaemupvD3n03d5Jw+fJwxhnH9pydeabrUPBX0DIpxoSF\nKvzvfy7x8t3Z0qAB/OUvbtLVGWcAruzDmtRj52ytWpW37EO9eu5n/fbbc3u1mjQJXEYmJeWQJWDm\nGJaEGWMKVKWKmxTcpk3e477b5f2Tsx9/dDcFHDni2pQrl3cJp1273P99rlCtLXpuwmjTJjfm/sYb\nLpOqXBluuIGDvfux6pTOpK4qR+q/c5Ou1atdsWUfX9mHrl1ttQsTXJaEGWOKLC4OWrVym7/MzNzC\nkb45Z6mp8Mknef9T88nIcD1jloSZklpw92QSxw7houw00mMaknb7MC7sUhkmTUI//xw5coRtZ3bg\ny27jeZcbWbqwGmsn5/boliuXW/bh2mvzln2Ij4/s92aiV0iTMBHpBryIu/V7vKqOLqDdDcB04DxV\nXRLKmIwxoRMbC82bu83foUPuXKDCDmlp4YnNRK8Fd0+m1asDiMdVHq2fvYGEcbfDOEiPaciEI0P4\nN7fyy8+nU2GdS6zOPRduuSW3V+vMM4NT9sGYoghZEiYiMcAY3Lpq6cBiEZmZc7u3f7uqwP3AN6GK\nxRgTWRUrujvAAq1D17Bh+OMx0SM7Gxr9a8jRBMxHgG2czGM919GkWTmeyUm27C5f4yWh/FFsC6xR\n1bUAIjIV6A6k5mv3BPB34JEQxmKMibCCFj23ZVJMcfzyi6t798YbsOFI4O7Uk9jBm5Ot9oPxrlAm\nYQnARr/9dNxaa0eJyLlAA1X9REQKTMJEZAAwAKBOnTpFWrncq6uwW1xF59XYvBoXeCu2hAQYNOhk\nxo8/le3bK3HyyQe54461JCRsxyMhGo/bvx+mT3fJ1xdfuHlcl10GmzY3oEGARGxzTEPqRyBOYwor\nYp2yIlIOeA7od6K2qjoWGAvQpk0bLcryCl5dhd3iKjqvxubVuMB7sSUnw5NP+sfVNGczJjBVWLjQ\nJV7vvOMWsz/9dNeDeuutrqD9z126wbyxea7bTxzrB4yyJMx4WiiTsE1AA7/9+jnHfKoCZwMp4oqn\n1AVmisg1NjnfGGPKti1bXFWJCRNcOZT4eFdD9fbboUMHv0XpDx/mzA3/YW/tRuzZpdTL3sjmmIas\nHzCKDq/YbbfG20KZhC0GzhCRJFzy1Qu42XdSVfcAtX37IpICPGwJmDHGlE2HDrlyJhMmuLVOs7Nd\nEfv/+z+XgFWtGuCiN96AtYrsSJ8AACAASURBVGup+vHHVL3yyqO9rNYDZkqDkCVhqpolIvcCs3El\nKiao6goRGQksUdWZoXptY4wxpcfy5S7xeustt/j8KafAI4+4pRvPOus4Fx46BE88AW3bwhVXhCtc\nY4ImpHPCVHUWMCvfsaEFtE0OZSzGGGO8Y/duePttN9dr8WKoUAGuucYNN156aSHLSEyY4ArNjR2L\nrQlkSiOrlmKMMSYsjhyBuXNd4vX++26FhebN4YUX3KoJtWuf+DmOysx0s/Pbt3dZmzGlkCVhxhhj\nQmr9erdu6KRJrmBvjRrQvz/cdpurXF+sTqxx4yA93c0Js14wU0pZEmaMMSboMjLcgu4TJrjeLxG4\n5BIYPdqtzRgbW4InP3AAnnoKOnWCzp2DFrMx4WZJmDHGmKBQdfO7Jkxw871+/x1OPdXNnb/11iAu\nUfXaa7B1qyscZr1gphSzJMwYY0yJbNvm7mycMAFSU6FyZVdS4rbb4KKLXGX7oNm/33Wnde3qntyY\nUsySMGOMMUV2+LCr5TVhgqvtlZUF7dq5GxV79oRq1UL0wmPGwPbtMGJEiF7AmPCxJMwYY0yhpabC\na6+dSq9ergesTh0YNMj1ejVpEuIX37sX/vEP6NYNLrwwxC9mTOhZEmaMMea49uxx068mToRFiyAm\npj5XX+1qenXr5mp8hcU//wk7d1ovmIkaloQZY4w5xpEj8MUXLvGaPt3dkNisGTz7LCQlLeS669qH\nN6A9e+CZZ+Cqq1yFfGOigCVhxhhjjkpLc6W3Jk6Edevc3K6+fV2vV5s27mbElJTD4Q/shRdg1y4Y\nOTL8r21MiATznhVjjCl1RKSbiPwkImtEZHCA8w1FZJ6IfCciy0Tkipzjl4jIUhH5Medrl/BHHxyZ\nmTB1qis8n5gIQ4e60hKTJ7tKEK++CuedF8FqELt2wXPPwXXXQatWEQrCmOCznjBjTJklIjHAGOAS\nIB1YLCIzVTXVr9lfgWmq+qqINMWth5sI/ApcraqbReRsYDaQENZvoARU4dtv3d2NU6a4tRwbNYJh\nw1zPV2JipCP089xzrujY8OGRjsSYoLIkzBhTlrUF1qjqWgARmQp0B/yTMAV8BReqA5sBVPU7vzYr\ngMoiUklVD4Y86hLYscP1cE2YAD/+6CrX33CDG25MTg5yTa9g2LnTDUXeeCO0aBHpaIwJKkvCjDFl\nWQKw0W8/HTg/X5vhwBwRuQ+IB7oGeJ4bgG8DJWAiMgAYAFCnTh1SUlIKHdy+ffuK1L4g2dnC//5X\nk08/PYWFC2uRlVWOxo1/Z9CgLXTpsoMqVbIAmD8/vHEVRtK4cTTcv5/FV1xBRiFeM5yxFYVX4wLv\nxubVuCB4sVkSZowxx9cbmKSqz4pIO+BNETlbVY8AiEgz4O/ApYEuVtWxwFiANm3aaHJycqFfOCUl\nhaK0z++nn9wE+3//G7ZsgZNOgvvvdzW9mjWrhuvgO6vIz1vSuApt+3b44APo3Zu2/foV6pKwxVZE\nXo0LvBubV+OC4MVmSZgxpizbBDTw26+fc8xff6AbgKouFJFYoDawXUTqAzOAW1X1lzDEe0J798K0\naS75+uoriImBK690ideVV4axplcw/OMf7q6BoUMjHYkxIWFJmDGmLFsMnCEiSbjkqxdwc742acDF\nwCQRaQLEAjtEpAbwCTBYVb8KY8zHUIUvv3SJ17RpkJEBjRu7HOaPf4S6dSMZXTFt2eKWKLrlFjir\n6L11xpQGloQZY8osVc0SkXtxdzbGABNUdYWIjASWqOpM4CFgnIgMwk3S76eqmnPd6cBQEfF11Vyq\nqtvDFX96uhtqnDgR1qyBqlWhTx83yf788yNYUiIY/v53t0Cl9YKZKGZJmDGmTFPVWbiyE/7Hhvo9\nTgWOKQ+vqk8CT4Y8wHwOHoSZM93djXPmuMr2yckuV7n+eoiPD3dEIZCeDq+9Bv36wWmnRToaY0LG\nkjBjjPGYyZNhyBBIS+tEw4YwapRbMmjCBHfut9+gQQPXpl8/V1g1qvztb5CdDX/9a6QjMSakLAkz\nxhgPmTwZBgxw87pA2LDBzetShUqVXNH422+HLl3cpPuok5YG48ZB//4eqxhrTPBZEmaMMR4yZIgv\nAculCjVrwi+/uK9RbdQoN5ltyJBIR2JMyHmtNrIxxpRpaWmBj+/eXQYSsLVr3ZjrgAFuvNWYKGdJ\nmDHGeEjDhkU7HlWefNKNsT72WKQjMSYsLAkzxhgPGTUK4uLyHouLc8ej2urVrt7GXXdBvXqRjsaY\nsLAkzBgTFUTkfRG5UkRK9edanz4wdiw0agQiSqNGbr9Pn0hHFmJPPAEVK8Kjj0Y6EmPCplR/WBlj\njJ9XcNXuV4vIaBEptWXW+/SB9eth7twvWL++DCRgq1a520LvvbeUlvc3pngsCTPGRAVV/Y+q9gHO\nBdYD/xGRr0XkNhEpTSsmlj0jRkDlyvDII5GOxJiwsiTMGBM1RKQW0A+4A/gOeBGXlH0ewbDM8Sxf\nDu+8AwMHwkknRToaY8IqpEmYiHQTkZ9EZI2IDA5w/k4R+VFEvheRBSLSNJTxGGOil4jMAL4E4oCr\nVfUaVX1HVe8DqkQ2OlOgESOgShV46KFIR2JM2IWsWKuIxABjgEuAdGCxiMzMWYfNZ4qqvpbT/hrg\nOaBbqGIyxkS1l1R1XqATqtom3MGYQvjhB5g+3S18WatWpKMxJuxC2RPWFlijqmtV9RAwFeju30BV\nf/fbjQc0hPEYY6JbUxGp4dsRkZoicnckAzInMGwYVK8OgwZFOhJjIiKUyxYlABv99tOB8/M3EpF7\ngAeBikCXQE8kIgOAAQB16tQhJSWl0EHs27evSO3DxeIqOq/G5tW4wLuxhSiuP6nqGN+Oqu4SkT/h\n7po0XrN0KXz4IYwcCTVqnLi9MVEo4mtH5nxojhGRm4G/An0DtBkLjAVo06aNJicnF/r5U1JSKEr7\ncLG4is6rsXk1LvBubCGKK0ZERFUVjk6JqBjsFzFBMmwY/OEPcP/9kY7EmIgJ5XDkJsB/8a/6OccK\nMhW4NoTxGGOi22fAOyJysYhcDLydc8x4zTffwCefuJIU1apFOhpjIiaUPWGLgTNEJAmXfPXCFVI8\nSkTOUNXVObtXAqsxxpjieRT4M3BXzv7nwPjIhWMKNHQo1K7tirMaU4aFLAlT1SwRuReYDcQAE1R1\nhYiMBJao6kzgXhHpChwGdhFgKNIYYwpDVY8Ar+ZsxqsWLIA5c+Dpp11pCmPKsJDOCVPVWcCsfMeG\n+j22yQDGmKAQkTOAvwFNgVjfcVU9NWJBmWMNGwZ16sDdduOqMVYx3xgTLSbiesGygM7Av4G3IhqR\nySslBebOhcGDIS4u0tEYE3GFSsJE5H4RqSbO6yLyrYhcGurgjDGmCCqr6n8BUdUNqjocN9fUeIGq\n6wWrVw/+/OdIR2OMJxS2J+z2nMKqlwI1gT8Co0MWlTHGFN1BESkHrBaRe0XkOmy5Iu/4739h/nz4\ny1/cYt3GmEInYZLz9QrgTVVd4XfMGGO84H7cupEDgdbALdjNPt6g6u6IrF8f7rgj0tEY4xmFnZi/\nVETmAEnAYyJSFTgSurCMMabwcgqz9lTVh4F9wG0RDsn4mz0bFi6E116DSpUiHY0xnlHYnrD+wGDg\nPFXNACpgH3LGGI9Q1WygQ3GuFZFuIvKTiKwRkcEBzjcUkXki8p2ILBORK/zOPZZz3U8iclkJvoXo\n5esFS0yE2+y/DWP8FbYnrB3wvaruF5FbgHOBF0MXljHGFNl3IjITeBfY7zuoqu8XdEFOD9oY4BLc\n+raLRWSmqqb6NfsrME1VXxWRpriyO4k5j3sBzYB6wH9E5MychND4fPwxLF4Mr78OFW0VKWP8FbYn\n7FUgQ0TOAR4CfsHd/m2MMV4RC+wEugBX52xXneCatsAaVV2rqodwy6d1z9dGAd/aOtWBzTmPuwNT\nVfWgqq4D1uQ8n/Hx9YKddhr88Y+RjsYYzylsT1iWqqqIdAdeVtXXRaR/KAMzxpiiUNXijHUlABv9\n9tOB8/O1GQ7MEZH7gHigq9+1i/Jdm5D/BURkADAAoE6dOqSkpBQ6uH379hWpfbgUNq7aX37J2d9/\nz8rBg9n21VehD4zS/55Fgldj82pcELzYCpuE7RWRx3ClKTrm3AZeocSvbowxQSIiE3G9Vnmo6u0l\nfOrewCRVfVZE2gFvisjZhb1YVccCYwHatGmjycnJhX7hlJQUitI+XAoV15EjMHAgnHUWTZ54gibl\nQ7pAS9FiiwCvxgXejc2rcUHwYivsb0VP3OLbt6vqVhFpCDxd4lc3xpjg+djvcSxwHblDhwXZBDTw\n26+fc8xff6AbgKouFJFYoHYhry27pk+HH3+EKVMgTAmYMaVNoeaEqepWYDJQXUSuAjJV1eaEGWM8\nQ1Xf89smAzcBbU5w2WLgDBFJEpGKuIn2M/O1SQMuBhCRJrgEb0dOu14iUklEkoAzgP8F7zsqxbKz\nYfhwaNoUbrop0tEY41mF+vNERG7C9Xyl4Iq0/lNEHlHV6SGMzRhjSuIM4OTjNVDVLBG5F5gNxAAT\nVHWFiIwElqjqTNzNSONEZBBuuLOfqiqwQkSmAam49SrvsTsjc7zzDqxcCdOmQUxMpKMxxrMK20c8\nBFcjbDuAiJwE/AewJMwY4wkispe8c8K2Ao+e6DpVnYUrO+F/bKjf41SgfQHXjgJGFSfeqJWVBSNG\nQIsWcMMNkY7GGE8rbBJWzpeA5dhJ4ctbGGNMyKlq1UjHYHBzwH7+GWbMgHL234Qxx1PY35DPRGS2\niPQTkX7AJ+T7y9EYYyJJRK4Tkep++zVE5NpIxlTmHD7sesFatYLu+cutGWPyK1RPmKo+IiI3kNsl\nP1ZVZ4QuLGOMKbJh/p9LqrpbRIYBH0QwprLl3/+GtWvho49AJNLRGON5hb5vWFXfA94LYSzGGFMS\ngXr2rTZCuBw6BE88AW3bwpVXRjoaY0qF435ABZjoevQUoKpaLcA5Y4yJhCUi8hxuLUiAe4ClEYyn\nbJk4ETZsgH/9y3rBjCmk4yZhNtHVGFOK3Ac8DryD++Pxc1wiZkItMxOefBIuvBAuvTTS0RhTalhX\nvTEmKqjqfmBwpOMok8aPh/R0mDTJesGMKQK7f9gYExVE5HMRqeG3X1NEZkcypjLhwAF46im46CLo\n0iXS0RhTqlhPmDEmWtRW1d2+HVXdJSLHrZhvguBf/4ItW+Dtt60XzJgisp4wY0y0OCIiDX07IpJI\n4BuLTLDs3w9/+xtcfDF06hTpaIwpdawnzBgTLYYAC0TkC9wd3B2BAZENKcq98gps3+4KtBpjisyS\nMGNMVFDVz0SkDS7x+g5XpPVAZKOKYnv3wt//DpddBu0DLq1pjDkBS8KMMVFBRO4A7gfqA98DFwAL\nAZstHgovvww7d8LIkZGOxJhSy+aEGWOixf3AecAGVe0MtAJ2H/8SUxwx+/fD00/DVVe5CvnGmGKx\nJMwYEy0yVTUTQEQqqeoq4KwIxxSV6k+fDrt22VwwY0oopEmYiHQTkZ9EZI2IHFNEUUQeFJFUEVkm\nIv8VkUahjMcYE9XSc+qEfQB8LiIfAhsiHFP02bWLBu++C9deC+eeG+lojCnVQjYnTERicGu4XQKk\nA4tFZKaqpvo1+w5oo6oZInIX8A+gZ6hiMsZEL1W9LufhcBGZB1QHPotgSNHp+ecpv3+/9YIZEwSh\n7AlrC6xR1bWqegiYCnT3b6Cq81Q1I2d3EW5CrTHGlIiqfqGqM3M+e0yw7NwJL7zA9k6doEWLSEdj\nTKkXyrsjE4CNfvvpwPnHad8f+DTQCREZQE69nzp16pCSklLoIPbt21ek9uFicRWdV2Pzalzg3di8\nGpc5gWeegX37WN+3L7YUgTEl54kSFSJyC9AGCFhyWVXHAmMB2rRpo8nJyYV+7pSUFIrSPlwsrqLz\namxejQu8G5tX4zLHsX07/POf0KsXGUlJkY7GmKgQyuHITUADv/36OcfyEJGuuErX16jqwRDGY4wx\nprieftot1j1sWKQjMSZqhDIJWwycISJJIlIR6AXM9G8gIq2Af+ESsO0hjMUYY0xxbd0KY8bALbfA\nWVb1w5hgCVkSpqpZwL3AbGAlME1VV4jISBG5JqfZ00AV4F0R+V5EZhbwdMYYYyJl9Gg4dAgefzzS\nkRgTVUI6J0xVZwGz8h0b6ve4ayhf3xhjTkREugEvAjHAeFUdne/880DnnN044GRVrZFz7h/Albg/\naD8H7ldVDVfsYbFpE7z2GvTtC6efHulojIkqnpiYb4wxkVCYeoaqOsiv/X245ZAQkQuB9oCvVsMC\n3M1FKWEJPlz+9jfIzoa//jXSkRgTdWzZImNMWXbCeob59AbeznmsQCxQEagEVAC2hTDW8EtLg3Hj\n4Pbbwe6INCboLAkzxpRlgeoZJgRqmLOsWhIwF0BVFwLzgC0522xVXRnSaMPtqafc1yFDIhuHMVHK\nhiONMaZwegHTVTUbQEROB5qQu9LH5yLSUVW/9L+otBabjt2yhbbjx7Pl6qtZvXYtrF3ribhOxKux\neTUu8G5sXo0LghebJWHGmLKsUPUMc/QC7vHbvw5YpKr7AETkU6AdkCcJK7XFpvv3h/LlSXj5ZRIS\n8nYOernYrldj82pc4N3YvBoXBC82G440xpRlJ6xnCCAijYGawEK/w2lAJxEpLyIVcJPyo2M4cs0a\neOMNuPNOSAg4OmuMCQJLwowxZVYh6xmCS86m5is/MR34BfgR+AH4QVU/ClPoofXEE1CxIgweHOlI\njIlqNhxpjCnTTlTPMGd/eIDrsoE/hzS4SFi1Ct56Cx58EOrWjXQ0xkQ16wkzxhiTa+RIqFwZ/u//\nIh2JMVHPkjBjjDHOihUwdSrcdx+cdFKkozEm6lkSZowxxhkxAqpUgYcfjnQkxpQJloQZY4yBH36A\nd9+FBx6AWrUiHY0xZYIlYcYYY2D4cKheHQYNOmFTY0xwWBJmjDFl3dKl8MEH7o7ImjUjHY0xZYYl\nYcYYU9YNH+6Sr/vvj3QkxpQploQZY0xZ9r//wccfwyOPuOFIY0zYWBJmjDFl2dChULs23HtvpCMx\npsyxJMwYY8qqr76C2bNdYdaqVSMdjTFljiVhxhhTVg0bBiefDHffHelIjCmTbO1IY4wpi774Av77\nX3j+eYiPj3Q0xpRJ1hNmjDFljaqbC3bKKfDn6FuD3JjSwnrCjDGmrJk7F+bPh3/+0y3WbYyJCOsJ\nM8aYssTXC1a/PtxxR6SjMaZMs54wY4wpS+bMga+/htdeg9jYSEdjTJlmPWHGGFNWqMLjj0OjRnDb\nbZGOxpgyz3rCjDGmrPjkE1i8GMaPh4oVIx2NMWWe9YQZY0xZ4JsLduqpcOutkY7GGIP1hBljTNnw\n4Yfw3XcwaRJUqBDpaIwxhLgnTES6ichPIrJGRAYHOH+RiHwrIlki0iOUsRhjTJl15Iirjn/mmdCn\nT6SjMcbkCFlPmIjEAGOAS4B0YLGIzFTVVL9maUA/4OFQxWGMMWXee+/BsmUweTKUtwEQY7wilL+N\nbYE1qroWQESmAt2Bo0mYqq7POXckhHEYY0zZlZ0Nw4dDkybQs2ekozHG+AllEpYAbPTbTwfOD+Hr\nGWOMyW/aNEhNhXfegZiYSEdjjPFTKvqlRWQAMACgTp06pKSkFPraffv2Fal9uFhcRefV2LwaF3g3\nNi/FJSLdgBeBGGC8qo7Od/55oHPObhxwsqrWyDnXEBgPNAAUuMLXw+8JWVmuF6x5c+hh026N8ZpQ\nJmGbcB9MPvVzjhWZqo4FxgK0adNGk5OTC31tSkoKRWkfLhZX0Xk1Nq/GBd6NzStxFWbuqqoO8mt/\nH9DK7yn+DYxS1c9FpArgrakVU6bAzz/D++9DOatIZIzXhPK3cjFwhogkiUhFoBcwM4SvZ4wxRXV0\n7qqqHgJ8c1cL0ht4G0BEmgLlVfVzAFXdp6oZoQ640A4fhpEjoVUruPbaSEdjjAkgZEmYqmYB9wKz\ngZXANFVdISIjReQaABE5T0TSgRuBf4nIilDFY4wxAQSau5oQqKGINAKSgLk5h84EdovI+yLynYg8\nndOz5g1vvgm//AIjRoBIpKMxxgQQ0jlhqjoLmJXv2FC/x4txw5TGGON1vYDpqpqds18e6IgbnkwD\n3sGV3Hnd/6JIzGmVw4dpO2QIhxs35tsqVSDI8++8NKcvP6/G5tW4wLuxeTUuCF5spWJivjHGhEhR\n5q72Au7x208Hvvcrw/MBcAH5krCIzGn9179g61YqT5xIcufOJ25fRF6Z0xeIV2Pzalzg3di8GhcE\nLzabqWmMKcsKNXdVRBoDNYGF+a6tISIn5ex3wa8OYsQcPAhPPgnt2sFll0U6GmPMcVhPmDGmzFLV\nLBHxzV2NASb45q4CS1TVl5D1Aqaqqvpdmy0iDwP/FREBlgLjwvwtHGv8eEhPh4kTbS6YMR5nSZgx\npkw70dzVnP3hBVz7OdAiZMEV1YED8NRT0LEjXHxxpKMxHnf48GHS09OpXr06K1eujHQ4x/BqXBA4\nttjYWOrXr0+FChUK/TyWhBljTLQYOxY2b3b1wawXzJxAeno6VatWpVatWlSrVi3S4Rxj7969VK1a\nNdJhBJQ/NlVl586dpKenk5SUVOjnsTlhxhgTDTIy4G9/gy5doFOnSEdjSoHMzExq1aqFWMJeYiJC\nrVq1yMzMLNJ11hNmjDHR4JVXYNs2mD490pGYUsQSsOApzntpPWHGGFPa7dsHf/87XHopdOgQ6WiM\nMYVkSZgxxpR2L78Mv/7qlikyJkQmT4bERLcMaWKi2w+nKlWqALB582Z6FLAgfXJyMkuWLDnu87zw\nwgtkZOSuMHbFFVewe/fu4AVaBJaEGWNMafb77/D003DllXD++ZGOxkSpyZNhwADYsAFU3dcBA8Kf\niAHUq1eP6SUYds+fhM2aNYsaNWoEI7QiszlhxhhTmr34Ivz2m1sj0phieuAB+P77gs8vWuTqAPvL\nyID+/WFcAdXxWraEF14o+DkHDx5MgwYNuOcetxDF8OHDKV++PPPmzWPXrl0cPHiQp556iu7du+e5\nbv369Vx11VUsX76cAwcOcNttt/HDDz/QuHFjDhw4cLTdXXfdxeLFizlw4AA9evRgxIgRvPTSS2ze\nvJnOnTtTu3Zt5s2bR2JiIkuWLKF27do899xzTJgwAYA77riDBx54gPXr13P55ZfToUMHvv76axIS\nEnjrrbeCcuem9YQZY0xptXs3PPssdO8OrVtHOhoTxfInYCc6Xhg9e/Zk2rRpR/enTZtG3759mTFj\nBt9++y2ffPIJDz30EH41ko/x6quvEhcXx8qVKxkxYgRLly49em7UqFEsWbKEZcuW8cUXX7Bs2TIG\nDhxIvXr1mDdvHvPmzcvzXEuXLmXixIl88803LFq0iHHjxvHdd98BsHr1au655x5WrFhBjRo1+PDD\nD4v/jfuxnjBjjCmtnn8e9uyxXjBTYsfrsQI3B2zDhmOPN2pU/PXhW7Vqxfbt29m8eTM7duygZs2a\n1K1bl0GDBjF//nwANm3axLZt26hbt27A55g/fz4DBw4EoEWLFrRokVs7edq0aYwdO5asrCy2bNlC\nampqnvP5LViwgOuuu474+HgArr/+er788kuuueYakpKSaNmyJQCtW7cmLS2teN90PpaEGWNMabRz\np0vCevSAc86JdDQmyo0a5eaA+U2lIi7OHS+JG2+8kenTp7N161Z69uzJ5MmT2bFjB0uXLiUzM5Pm\nzZsXufYWwLp163jmmWdYvHgxNWvWpF+/fsV6Hp9KlSodfRwTE0NWVlaxn8ufDUcaY0xp9OyzrjTF\nsGGRjsSUAX36uAUZGjVyizE0auT2+/Qp2fP27NmTqVOnMn36dG688Ub27NnDySefTIUKFZg/fz4b\nAnW/+bnooouYMmUKAMuXL2fZsmUA/P7778THx1O9enW2bdvGp59+evSaqlWrsnfv3mOeq2PHjnzw\nwQdkZGSwf/9+ZsyYQceOHUv2DZ6A9YQZY0xps2MHvPQS9OwJZ58d6WhMGdGnT8mTrvyaNWvG3r17\nSUhI4JRTTqFPnz5cffXVNG/enHPOOYfGjRsf9/q77rqL2267jSZNmtCkSRNa58yNPOecc2jVqhWN\nGzemQYMGtG/f/ug1AwYMoFu3bkfnhvmce+659OvXj7Zt2wJuYn6rVq1Yv359cL9pP5aEGWNMafP0\n026xbusFM1Hgxx9/PPq4du3aLFy4EDh2fcZ9+/YBkJiYyPLlywGoXLkyU6dODfi8kyZNCnj8vvvu\n47777ju6759kPfjggzz44IN52vu/HsDDDz8csCetOGw40hhjSpNt21xx1j594AS9BMYYb7MkzBhj\nSpPRo+HQIXj88UhHYowpIUvCjDGmtNi8GV59FW69Fc44I9LRGGNKyJIwY4wpLf72N8jOtl4wY6KE\nJWHGGFMabNzoagLcfjskJUU6GmNMEFgSZowxpcGoUW7l5CFDIh2JMSZILAkzxhivW78eXn8d/vQn\naNgw0tGYsmryZLd+Ubly7uvkySV6ut27d/PKK68U+borrriC3bt3H7fN0KFD+c9//lPc0MLGkjBj\njPG6J5+EmBj4y18iHYkpqyZPdusWbdjgemQ3bHD7JUjECkrCTrQk0KxZs6hRo8Zx24wcOZKuXbsW\nO7ZwsWKtxhjjZb/8ApMmwb33QkJCpKMx0eqBB+D77ws+v2gRHDyY91hGBvTvD+PGBb6mZcvjrgw+\nePBgfvnlF1q2bEmFChWIjY2lZs2arFq1ip9//pnevXuzZcsWMjMzuf/++xkwYADgiqcuWbKEffv2\ncfnll9OhQwe+/vprEhIS+PDDD6lcuTL9+vXjqquuokePHiQmJtK3b18++ugjDh8+zLvvvkvjxo3Z\nsWMHN998M5s3b6Zdu3Z8/vnnLF26lNq1axf13Ss26wkzxhgvGzkSKlaEwYMjHYkpy/InYCc6Xgij\nR4/mtNNO4/vvv+fpbXb96QAADctJREFUp5/m22+/5cUXX+Tnn38GYMyYMSxdupQlS5bw0ksvsXPn\nzmOeY/Xq1dxzzz2sWLGCGjVq8N577wV8rdq1a/Ptt99y11138cwzzwAwYsQIunTpwooVK+jRowdp\naWnF/l6KK3p7wiZPhiFD6JSW5uZQjBoV/EWvjDEmhCqnpcFbb8GgQVC3bqTDMdHsOD1WgJsDFmgx\n7UaNICUlKCG0bduWJL87f1977TVmzZoFwMaNG1m9ejW1atXKc01SUhItW7YEoHXr1gWu83j99dcf\nbfP+++8DsGDBAmbMmAFAt27dqFmzZlC+j6KIzp4wv7Fr8Y1d/+lP8M9/wpYtbvHb336DPXtg/37I\nzITDh904d1mVM+GyU5cuQZlwWSbYe1Z0HnzPRKSbiPwkImtE5JjuJhF5XkS+z9l+FpHd+c5XE5F0\nEXk5aEHlvE9t+/Z1n0unnRa0pzamWEaNgri4vMfi4tzxIImPjz/6OCUlhZSUFBYuXMgPP/xAq1at\nyMzMPOaaSpUqHX0cExNT4HwyX7vjtYmEkPaEiUg34EUgBhivqqPzna8E/BtoDewEeqrq+hK/8JAh\nbqza34EDMHCg244ftJsAW768++r/ONCxYrZtsnOnu9spyM9brLazZ7uJv5mZCLik9Y47YOtW6N7d\nvSe+98b3NQKPYzIyIGcB16A9f3H5Ev2MjNz3LGe+gvW4FsCD75mIxABjgEuAdGCxiMxU1VRfG1Ud\n5Nf+PqBVvqd5ApgftKDyv0+q8PDDUK2a/WyZyPH97A0ZAkEaYapatWqBC2Hv2bOHGjVqEBcXx6pV\nq1i0aFGxX6cg7du3Z9q0aTz66KPMmTOHXbt2Bf01TiRkSVhhPtyA/sAuVT1dRHoBfwd6lvjFjzeu\n++qrruJ0Vpb7WpjHxW17+LDrZQvQtuq+fbBuXeGfN9wyM90H/8MPh/+1C9AxVE9cnAQuM/PYntOM\nDPjjH+Guu/I+b/7XKMyx4lwT4Fi7Q4fA95dimF/7mPNr1rifZ38ZGe5DPXLJRVtgjaquBRCRqUB3\nILWA9r2BYb4dEWkN1AE+A9oEJaJAf0RG/n0yxv38BfFnsFatWrRv356zzz6bypUrU6dOnaPnunXr\nxssvv0yTJk0466yzuOCCC4L2uj7Dhg2jd+/evPnmm7Rr1466detStWrVoL/O8YSyJ6wwH27dgeE5\nj6cDL4uIqJZwXLBhw4LHru+8s0RPHSz/S0khOTm5cI1V4ciR0CWN3bsXPBT773/nxuBr4/81jI9/\nWbOG0047LeyvG/Dx008Hfr9UXS+i//uZ/3070bHiXFPA8/y2eTOnnHJKRF77mPOrVhFQBCbD+kkA\nNvrtpwPnB2ooIo2AJGBuzn454FngFiB498IX9H5E9n0yJiSmTJny/+3dX4xcdRnG8e9Du+xSl2yR\npZWwZVuUoGCUUtIUiIaEmBAvioklNraVkhgSkSgXJIpRVK68IEoUEyBC2tJGQCymkpKK9I8haaEE\ni7QFzEKILCEUF/tn1UXavl6cs2WYzuzubGfO+e3s80kmmXPm15ln3515+9tzzpxTc31nZycbN26s\nOSkaPe6rt7eXvXv3nlh/W8VGgzVr1pw0HuDyyy9ne34MW09PD1u2bGHmzJns3LmT3bt3f2T3ZhFa\nOQmbSHM7MSYijko6BJwN/LNykKSbgJsA5s6de6KA9cxZuZKL7rqLGRXf2jjW2cmrK1dyoEkHEJ6q\n4eHhcX+OljnttOzbVrklc+bQ9c47Jw0bmTuXXfPmFZlsTMMXXcSb3d1lxwBgybp19Wu2dGkJiWob\nHh6mO5Wa7dhRu2Zz5rArkc/lOJYDj0XE6Kbpm4HNETGoMXZvN9q/6n4eE6pTqf1rHKlmSzFXT08P\nR44c4dixY3V3C5ap1bkGBgZYvXo1x48fp6Ojg7vvvnvCr1cv28jISGO/54hoyQ1YRnYc2OjyKuCe\nqjF7gb6K5deA3rGed9GiRTEh69dH9PfHcSmivz9bTsi2bdvKjvCh9esjZs0a3c6T3WbNcs3G4po1\n7hRqBjwfrelTVwBbKpZvB26vM/avwJUVyxuAfwBvkP3heBj42VivN6H+NQXeW0m9r6qkmi3FXPv3\n74+IiMOHD5ecpLZUc0XUzzZa00pj9a9WfjvyLaByM0pfvq7mGEkzgR6yA/RP3YoV8MYb7Ni6Nbvk\nh4+lqG/FiuzCwP39hJTttr3/ftdsLK5Z49Ks2W7gQkkLJJ1OtrVrU/UgSZ8GzgJ2jq6LiBURcX5E\nzAduA9ZFxKmfzCvNOlmbispDCOyUTKaWrZyETaS5bQJuyO8vA7aG3xHl8KS1ca5Z4xKrWUQcBW4B\ntgAvA49GxD5Jd0qq3K+8HHi4sP6UWJ2sPXV1dTE0NOSJWBNEBENDQ3R1dTX071p2TFhkx3iNNrcZ\nwIOjzY1s09wm4AHgIUkDwHtkjc7MrDARsRnYXLXujqrln4zzHGuANU2OZtZSfX19DA4OcvDgwYYn\nD0UYGRlJMhfUztbV1UVfX19Dz9PS84SN19wiYgS4vpUZzMzM7GQdHR0sWLCA7du3s3Bh9envypdq\nLmhetvY8Y76ZmZlZ4jwJMzMzMyuBJ2FmZmZmJdBU+1aEpHeBGqfDr6uXqpO/JsK5GpdqtlRzQbrZ\nGs3VHxHntCpMUdy/CpFqtlRzQbrZUs0FjWWr27+m3CSsUZKej4jmXNOtiZyrcalmSzUXpJst1Vyp\nSbVOqeaCdLOlmgvSzZZqLmheNu+ONDMzMyuBJ2FmZmZmJZgOk7D7yw5Qh3M1LtVsqeaCdLOlmis1\nqdYp1VyQbrZUc0G62VLNBU3K1vbHhJmZmZmlaDpsCTMzMzNLjidhZmZmZiVoi0mYpGslvSppQNL3\nazzeKemR/PFnJc1PKNtqSe9K2pPfvllQrgclHZC0t87jkvTLPPffJF2WSK6rJR2qqNcdtca1INc8\nSdsk7Ze0T9J3a4wpq2YTyVZ43SR1SXpO0ot5rp/WGFPaZzMlqfYw96+m53L/mly29u1fETGlb8AM\n4DXgAuB04EXg4qoxNwP35veXA48klG01cE8JdfsicBmwt87jXwaeBAQsAZ5NJNfVwBMl1Otc4LL8\n/pnA32v8Lsuq2USyFV63vA7d+f0O4FlgSdWYUj6bKd1S7WHuXy3J5f41uWxt27/aYUvYYmAgIl6P\niP8BDwPXVY25Dlib338MuEaSEslWioj4C/DeGEOuA9ZFZhcwW9K5CeQqRUS8HREv5PePAC8D51UN\nK6tmE8lWuLwOw/liR36r/iZQWZ/NlKTaw9y/mp+rFO5fjSuqf7XDJOw84M2K5UFO/gWeGBMRR4FD\nwNmJZAP4ar759zFJ8wrINRETzV6GK/JNxE9KuqToF883OS8k+8uoUuk1GyMblFA3STMk7QEOAE9F\nRN2aFfzZTEmqPcz9qzXcv+qYjv2rHSZhU90fgfkR8TngKT6cVVttL5Bdh+vzwK+APxT54pK6gd8D\nt0bE4SJfezzjZCulbhFxLCIuBfqAxZI+W8TrWmHcvxrj/lXHdO1f7TAJewuo/OurL19Xc4ykmUAP\nMJRCtogYioj388XfAIsKyDURE6lr4SLi8Ogm4ojYDHRI6i3itSV1kDWJDRGxscaQ0mo2XrYy65a/\n5kFgG3Bt1UNlfTZTkmoPc/9qMvevyWVr5/7VDpOw3cCFkhZIOp3s4LhNVWM2ATfk95cBWyM/kq7s\nbFX73JeS7Q9PwSbgG/k3ZpYAhyLi7bJDSfrE6D53SYvJ3sMt/087f80HgJcj4ud1hpVSs4lkK6Nu\nks6RNDu/fwbwJeCVqmFlfTZTkmoPc/9qMvevyWVr5/4181SDli0ijkq6BdhC9m2eByNin6Q7gecj\nYhPZL/ghSQNkB00uTyjbdyQtBY7m2VYXkU3Sb8m+cdIraRD4MdmBh0TEvcBmsm/LDAD/AW5MJNcy\n4FuSjgL/BZYX9J/2VcAq4KX8GAGAHwDnV2QrpWYTzFZG3c4F1kqaQdY0H42IJ1L4bKYk1R7m/tWS\nXO5fk8vWtv3Lly0yMzMzK0E77I40MzMzm3I8CTMzMzMrgSdhZmZmZiXwJMzMzMysBJ6EmZmZmZXA\nkzBrG5KulvRE2TnMzCbDPWz68STMzMzMrASehFnhJK2U9JykPZLuyy+SOizpF5L2SXpa0jn52Esl\n7VJ2geDHJZ2Vr/+UpD8ru6DrC5I+mT99t7ILCb8iacPoWZbNzJrFPcyaxZMwK5SkzwBfA67KL4x6\nDFgBfIzsLMSXADvIzjQNsA74Xn6B4Jcq1m8Afp1f0PVKYPTyGguBW4GLgQvIzsZsZtYU7mHWTFP+\nskU25VxDdpHf3fkfeGcAB4DjwCP5mPXARkk9wOyI2JGvXwv8TtKZwHkR8ThARIwA5M/3XEQM5st7\ngPnAM63/scxsmnAPs6bxJMyKJmBtRNz+kZXSj6rGTfZ6Wu9X3D+G3+Nm1lzuYdY03h1pRXsaWCZp\nDoCkj0vqJ3svLsvHfB14JiIOAf+S9IV8/SpgR0QcAQYlfSV/jk5Jswr9KcxsunIPs6bxDNsKFRH7\nJf0Q+JOk04APgG8D/wYW548dIDvmAuAG4N68Qb0O3JivXwXcl1/R/gPg+gJ/DDObptzDrJkUMdkt\npmbNI2k4IrrLzmFmNhnuYTYZ3h1pZmZmVgJvCTMzMzMrgbeEmZmZmZXAkzAzMzOzEngSZmZmZlYC\nT8LMzMzMSuBJmJmZmVkJ/g9SqSvtrO8pggAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXlKZ2eQj9Y7",
        "colab_type": "text"
      },
      "source": [
        "### 4. Optional: The CIFAR 10 Dataset (torchvision.datasets.CIFAR10). (30pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZjEdGaYojsg",
        "colab_type": "text"
      },
      "source": [
        "Include the code for defining, and training the neural network, as well as plots for training, validation for loss and accuracy. Show the predictions for one example of this dataset. Write down below your code in bold face the accuracy you obtained and in how many epochs. I will post the highest accuracies that people obtained for this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feWB2DO7lWQv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4c7cf0ce507d4e62a2205dfab8f2c1cd"
          ]
        },
        "outputId": "5c5adfcb-7c15-4013-d041-a142f9a01cbe"
      },
      "source": [
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "image_index = 200  # Feel free to change this.\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "# The 10 categories of objects in FashionMNIST.\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Load the dataset.\n",
        "trainset = CIFAR10(root = './data', train = True, transform = transform, download = True)\n",
        "valset = CIFAR10(root = './data', train = False, transform = transform, download = True)\n",
        "\n",
        "batchSize = 100\n",
        "# Shuffling is needed in case dataset is not shuffled by default.\n",
        "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size = batchSize, shuffle = True)\n",
        "# We don't need to bach the validation set but let's do it anyway.\n",
        "val_loader = torch.utils.data.DataLoader(dataset = valset,batch_size = batchSize, shuffle = False) # No need.\n",
        "\n",
        "# Define a learning rate. \n",
        "learningRate = 1e-3\n",
        "\n",
        "# Define number of epochs.\n",
        "N = 5\n",
        "# Setup the number of inputs, hidden neurons, and outputs.\n",
        "nInputs = 3 * 32 * 32\n",
        "nHidden = 512\n",
        "nOutputs = 10\n",
        "\n",
        "# Create the model here.\n",
        "linear_fn1 = toynn_Linear(nInputs, nHidden)\n",
        "relu_fn = toynn_ReLU()\n",
        "linear_fn2 = toynn_Linear(nHidden, nOutputs)\n",
        "loss_fn = toynn_CrossEntropyLoss()\n",
        "\n",
        "# log accuracies and losses.\n",
        "train_accuracies = []; val_accuracies = []\n",
        "train_losses = []; val_losses = []\n",
        "\n",
        "# Training loop. Please make sure you understand every single line of code below.\n",
        "# Go back to some of the previous steps in this lab if necessary.\n",
        "for epoch in range(0, N):\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    \n",
        "    # Make a pass over the training data.\n",
        "    for (i, (inputs, labels)) in enumerate(train_loader):\n",
        "        inputs = inputs.view(batchSize, 3 * 32 * 32)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "\n",
        "        # Make predictions.\n",
        "        a = linear_fn1.forward(inputs)\n",
        "        z = relu_fn.forward(a)\n",
        "        scores = linear_fn2.forward(z)\n",
        "        cum_loss += loss_fn.forward(scores, labels).item()\n",
        "        \n",
        "        # Count how many correct in this batch.\n",
        "        max_scores, max_labels = scores.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "        \n",
        "        #Backward pass. (Gradient computation stage)\n",
        "        scores_grads = loss_fn.backward(scores, labels)\n",
        "        linear2_grad = linear_fn2.backward(z, scores_grads)\n",
        "        relu_grad = relu_fn.backward(a, linear2_grad)\n",
        "        grad_inputs = linear_fn1.backward(inputs, relu_grad)\n",
        "        \n",
        "        # Parameter updates (SGD step).\n",
        "        linear_fn1.weight.add_(-learningRate, linear_fn1.weight_grads)\n",
        "        linear_fn1.bias.add_(-learningRate, linear_fn1.bias_grads)\n",
        "        linear_fn2.weight.add_(-learningRate, linear_fn2.weight_grads)\n",
        "        linear_fn2.bias.add_(-learningRate, linear_fn2.bias_grads)\n",
        "        \n",
        "        \n",
        "        # Logging the current results on training.\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "                  (epoch, i + 1, cum_loss / (i + 1), correct / (i * batchSize + 1)))\n",
        "    \n",
        "    train_accuracies.append(correct / len(trainset))\n",
        "    train_losses.append(cum_loss / len(trainset))\n",
        "    \n",
        "    \n",
        "    # Make a pass over the validation data.\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    for (i, (inputs, labels)) in enumerate(val_loader):\n",
        "        inputs = inputs.view(batchSize, 3 * 32 * 32)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        # Make predictions.\n",
        "        a = linear_fn1.forward(inputs)\n",
        "        z = relu_fn.forward(a)\n",
        "        scores = linear_fn2.forward(z)\n",
        "        cum_loss += loss_fn.forward(scores, labels).item()\n",
        "\n",
        "        # Count how many correct in this batch.\n",
        "        max_scores, max_labels = scores.max(1)\n",
        "        correct += (max_labels == labels).sum().item()\n",
        "          \n",
        "    val_accuracies.append(correct / len(valset))\n",
        "    val_losses.append(cum_loss / (i + 1))\n",
        "            \n",
        "    # Logging the current results on validation.\n",
        "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "          (epoch, cum_loss / (i + 1), correct / len(valset)))\n",
        "    \n",
        "\n",
        "plt.figure(figsize = (10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(val_losses, 'bo-', label = 'val-loss')\n",
        "plt.plot(train_losses, 'ro-', label = 'train-loss')\n",
        "plt.grid('on')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['validation', 'training'], loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, 'bo-', label = 'val-acc')\n",
        "plt.plot(train_accuracies, 'ro-', label = 'train-acc')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid('on')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['validation', 'training'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#prediction of an image \n",
        "\n",
        "# Setup the input variable x.\n",
        "img, label = trainset[image_index]\n",
        "x = img.view(1, 3 * 32 * 32)\n",
        "\n",
        "# Make predictions.\n",
        "a = linear_fn1.forward(x)\n",
        "z = relu_fn.forward(a)\n",
        "scores = linear_fn2.forward(z)\n",
        "score = scores.softmax(dim = 1)\n",
        "\n",
        "#print(score) # final prediction    \n",
        "\n",
        "# print the prediction\n",
        "max_label = score.max(1)\n",
        "print(max_label.indices.item()) \n",
        "\n",
        "print('actual label: Image {0} is a {1}'.format(image_index, classes[label]))\n",
        "print('prediction label: Image {0} is a {1}'.format(image_index, classes[max_label.indices]))     \n",
        "\n",
        "plt.imshow(img[0]); plt.axis('off'); plt.grid(False)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c7cf0ce507d4e62a2205dfab8f2c1cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Train-epoch 0. Iteration 00100, Avg-Loss: 2.1460, Accuracy: 0.2032\n",
            "Train-epoch 0. Iteration 00200, Avg-Loss: 2.0705, Accuracy: 0.2445\n",
            "Train-epoch 0. Iteration 00300, Avg-Loss: 2.0184, Accuracy: 0.2659\n",
            "Train-epoch 0. Iteration 00400, Avg-Loss: 1.9816, Accuracy: 0.2797\n",
            "Train-epoch 0. Iteration 00500, Avg-Loss: 1.9513, Accuracy: 0.2912\n",
            "Validation-epoch 0. Avg-Loss: 1.8240, Accuracy: 0.3493\n",
            "Train-epoch 1. Iteration 00100, Avg-Loss: 1.8076, Accuracy: 0.3510\n",
            "Train-epoch 1. Iteration 00200, Avg-Loss: 1.7894, Accuracy: 0.3577\n",
            "Train-epoch 1. Iteration 00300, Avg-Loss: 1.7820, Accuracy: 0.3605\n",
            "Train-epoch 1. Iteration 00400, Avg-Loss: 1.7707, Accuracy: 0.3668\n",
            "Train-epoch 1. Iteration 00500, Avg-Loss: 1.7627, Accuracy: 0.3691\n",
            "Validation-epoch 1. Avg-Loss: 1.7617, Accuracy: 0.3536\n",
            "Train-epoch 2. Iteration 00100, Avg-Loss: 1.7087, Accuracy: 0.3940\n",
            "Train-epoch 2. Iteration 00200, Avg-Loss: 1.7066, Accuracy: 0.3934\n",
            "Train-epoch 2. Iteration 00300, Avg-Loss: 1.6967, Accuracy: 0.3965\n",
            "Train-epoch 2. Iteration 00400, Avg-Loss: 1.6949, Accuracy: 0.3970\n",
            "Train-epoch 2. Iteration 00500, Avg-Loss: 1.6841, Accuracy: 0.4016\n",
            "Validation-epoch 2. Avg-Loss: 1.6150, Accuracy: 0.4275\n",
            "Train-epoch 3. Iteration 00100, Avg-Loss: 1.6399, Accuracy: 0.4232\n",
            "Train-epoch 3. Iteration 00200, Avg-Loss: 1.6418, Accuracy: 0.4193\n",
            "Train-epoch 3. Iteration 00300, Avg-Loss: 1.6377, Accuracy: 0.4188\n",
            "Train-epoch 3. Iteration 00400, Avg-Loss: 1.6341, Accuracy: 0.4208\n",
            "Train-epoch 3. Iteration 00500, Avg-Loss: 1.6277, Accuracy: 0.4225\n",
            "Validation-epoch 3. Avg-Loss: 1.5827, Accuracy: 0.4386\n",
            "Train-epoch 4. Iteration 00100, Avg-Loss: 1.5822, Accuracy: 0.4461\n",
            "Train-epoch 4. Iteration 00200, Avg-Loss: 1.5862, Accuracy: 0.4392\n",
            "Train-epoch 4. Iteration 00300, Avg-Loss: 1.5887, Accuracy: 0.4362\n",
            "Train-epoch 4. Iteration 00400, Avg-Loss: 1.5922, Accuracy: 0.4367\n",
            "Train-epoch 4. Iteration 00500, Avg-Loss: 1.5844, Accuracy: 0.4389\n",
            "Validation-epoch 4. Avg-Loss: 1.5524, Accuracy: 0.4525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAEKCAYAAABXMPIIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREYuT/AYy9pj1oTyqZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "actual label: Image 200 is a frog\n",
            "prediction label: Image 200 is a bird\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VLqNpM79-3l",
        "colab_type": "text"
      },
      "source": [
        "**I obtained a validation accuracy of 45% after 5 epochs.**"
      ]
    }
  ]
}